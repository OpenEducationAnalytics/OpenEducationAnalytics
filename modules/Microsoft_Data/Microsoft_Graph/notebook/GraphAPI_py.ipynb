{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import LongType\r\n",
        "import datetime\r\n",
        "import logging\r\n",
        "\r\n",
        "class GraphAPI(BaseOEAModule):\r\n",
        "    def __init__(self, source_folder='graph_api'):\r\n",
        "        BaseOEAModule.__init__(self, source_folder)\r\n",
        "\r\n",
        "        self.stage1np_graphapi_users = self.stage1np + '/users'\r\n",
        "        self.stage1np_graphapi_m365 = self.stage1np + '/m365_app_user_detail'\r\n",
        "        self.stage1np_graphapi_teams = self.stage1np + '/teams_activity_user_detail'\r\n",
        "        self.stage1np_graphapi_signInLogs = self.stage1np + '/signin_logs'\r\n",
        "\r\n",
        "        self.schemas['users'] = [['surname', 'string', 'mask'],\r\n",
        "                                ['givenName', 'string', 'mask'],\r\n",
        "                                ['userPrincipalName', 'string', 'hash'],\r\n",
        "                                ['id', 'string', 'mask'],\r\n",
        "                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
        "\r\n",
        "        self.schemas['m365'] = [['reportRefreshDate', 'date', 'no-op'],\r\n",
        "                                ['userPrincipalName', 'string', 'hash'],\r\n",
        "                                ['lastActivationDate', 'date', 'no-op'],\r\n",
        "                                ['lastActivityDate', 'date', 'no-op'],\r\n",
        "                                ['reportPeriod', 'long', 'no-op'],\r\n",
        "                                ['mobile', 'boolean', 'no-op'],\r\n",
        "                                ['web', 'boolean', 'no-op'],\r\n",
        "                                ['mac', 'boolean', 'no-op'],\r\n",
        "                                ['windows', 'boolean', 'no-op'],\r\n",
        "                                ['excel', 'boolean', 'no-op'],\r\n",
        "                                ['excelMac', 'boolean', 'no-op'],\r\n",
        "                                ['excelMobile', 'boolean', 'no-op'],\r\n",
        "                                ['excelWeb', 'boolean', 'no-op'],\r\n",
        "                                ['excelWindows', 'boolean', 'no-op'],\r\n",
        "                                ['oneNote', 'boolean', 'no-op'],\r\n",
        "                                ['oneNoteMac', 'boolean', 'no-op'],\r\n",
        "                                ['oneNoteMobile', 'boolean', 'no-op'],\r\n",
        "                                ['oneNoteWeb', 'boolean', 'no-op'],\r\n",
        "                                ['oneNoteWindows', 'boolean', 'no-op'],\r\n",
        "                                ['outlook', 'boolean', 'no-op'],\r\n",
        "                                ['outlookMac', 'boolean', 'no-op'],\r\n",
        "                                ['outlookMobile', 'boolean', 'no-op'],\r\n",
        "                                ['outlookWeb', 'boolean', 'no-op'],\r\n",
        "                                ['outlookWindows', 'boolean', 'no-op'],\r\n",
        "                                ['powerPoint', 'boolean', 'no-op'],\r\n",
        "                                ['powerPointMac', 'boolean', 'no-op'],\r\n",
        "                                ['powerPointMobile', 'boolean', 'no-op'],\r\n",
        "                                ['powerPointWeb', 'boolean', 'no-op'],\r\n",
        "                                ['powerPointWindows', 'boolean', 'no-op'],\r\n",
        "                                ['teams', 'boolean', 'no-op'],\r\n",
        "                                ['teamsMac', 'boolean', 'no-op'],\r\n",
        "                                ['teamsMobile', 'boolean', 'no-op'],\r\n",
        "                                ['teamsWeb', 'boolean', 'no-op'],\r\n",
        "                                ['teamsWindows', 'boolean', 'no-op'],\r\n",
        "                                ['word', 'boolean', 'no-op'],\r\n",
        "                                ['wordMac', 'boolean', 'no-op'],\r\n",
        "                                ['wordMobile', 'boolean', 'no-op'],\r\n",
        "                                ['wordWeb', 'boolean', 'no-op'],\r\n",
        "                                ['wordWindows', 'boolean', 'no-op'],\r\n",
        "                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
        "\r\n",
        "        self.schemas['teams'] = [['reportRefreshDate', 'date', 'no-op'],\r\n",
        "                                ['lastActivityDate', 'date', 'no-op'],\r\n",
        "                                ['deletedDate', 'string', 'no-op'],\r\n",
        "                                ['isDeleted', 'boolean', 'no-op'],\r\n",
        "                                ['isLicensed', 'boolean', 'no-op'], \r\n",
        "                                ['reportPeriod', 'long', 'no-op'],\r\n",
        "                                ['userPrincipalName', 'string', 'hash'],\r\n",
        "                                ['privateChatMessageCount', 'integer', 'no-op'],\r\n",
        "                                ['teamChatMessageCount', 'integer', 'no-op'],\r\n",
        "                                ['meetingsAttendedCount', 'integer', 'no-op'],\r\n",
        "                                ['meetingCount', 'integer', 'no-op'],\r\n",
        "                                ['meetingsOrganizedCount', 'integer', 'no-op'],                        \r\n",
        "                                ['callCount', 'integer', 'no-op'],\r\n",
        "                                ['audioDuration', 'integer', 'no-op'],\r\n",
        "                                ['videoDuration', 'integer', 'no-op'],\r\n",
        "                                ['screenShareDuration', 'integer', 'no-op'],                        \r\n",
        "                                ['scheduledOneTimeMeetingsAttendedCount', 'integer', 'no-op'],\r\n",
        "                                ['scheduledOneTimeMeetingsOrganizedCount', 'integer', 'no-op'],\r\n",
        "                                ['scheduledRecurringMeetingsAttendedCount', 'integer', 'no-op'],\r\n",
        "                                ['scheduledRecurringMeetingsOrganizedCount', 'integer', 'no-op'],\r\n",
        "                                ['adHocMeetingsAttendedCount', 'integer', 'no-op'],\r\n",
        "                                ['adHocMeetingsOrganizedCount', 'integer', 'no-op'],\r\n",
        "                                ['assignedProducts', 'string', 'no-op'],\r\n",
        "                                ['hasOtherAction', 'boolean', 'no-op'],\r\n",
        "                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
        "\r\n",
        "        self.schemas['sign_in_logs'] = [['id', 'string', 'mask'],\r\n",
        "                                ['createdDateTime', 'timestamp', 'no-op'],\r\n",
        "                                ['userPrincipalName', 'string', 'hash'],\r\n",
        "                                ['ipAddress', 'string', 'mask'], \r\n",
        "                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
        "    \r\n",
        "    def ingest(self):\r\n",
        "        \"\"\" Processes graphapi data from stage1 into stage2 using structured streaming within the defined functions below. \"\"\"\r\n",
        "        logger.info(\"Processing microsoft_graph data from: \" + self.stage1np)\r\n",
        "\r\n",
        "        items = mssparkutils.fs.ls(self.stage1np)\r\n",
        "        for item in items:\r\n",
        "            if item.name == \"users\":\r\n",
        "                self._process_graphapi_users_stage1_data()\r\n",
        "            elif item.name == \"m365_app_user_detail\":\r\n",
        "                self._process_graphapi_m365_stage1_data()\r\n",
        "            elif item.name == \"teams_activity_user_detail\":\r\n",
        "                self._process_graphapi_teams_stage1_data()\r\n",
        "            elif item.name == \"signin_logs\":\r\n",
        "                self._process_graphapi_signInLogs_stage1_data()\r\n",
        "            else:\r\n",
        "                logger.info(\"No defined function for processing this queried data\")\r\n",
        "        \r\n",
        "        logger.info(\"Finished processing graphapi data from stage 1 to stage 2\")\r\n",
        "\r\n",
        "    def _process_graphapi_users_stage1_data(self):\r\n",
        "        \"\"\" Processes users data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
        "        logger.info(\"Processing microsoft_graph users data from: \" + self.stage1np_graphapi_users)\r\n",
        "\r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        # read in the raw data, and explode the \"value\" array\r\n",
        "        df = spark.readStream.format('json').load(self.stage1np_graphapi_users + '/*/*.json', header='true')\r\n",
        "        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
        "        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
        "        currentDate = datetime.datetime.now()\r\n",
        "        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
        "            # create a new column for partitioning the folder structure\r\n",
        "        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
        "        # use the users_spark_schema for pseudonymization\r\n",
        "        users_spark_schema = oea.to_spark_schema(self.schemas['users'])\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['users'])\r\n",
        "\r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_users + '/_checkpoints_p').partitionBy('ReportYearMonth')\r\n",
        "            query = query.start(self.stage2p + '/users_pseudo')\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query.lastProgress)\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_users + '/_checkpoints_np').partitionBy('ReportYearMonth')\r\n",
        "            query2 = query2.start(self.stage2np + '/users_lookup')\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query2.lastProgress)      \r\n",
        "\r\n",
        "    def _process_graphapi_m365_stage1_data(self):\r\n",
        "        \"\"\" Processes m365 data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
        "        logger.info(\"Processing microsoft_graph m365 data from: \" + self.stage1np_graphapi_m365)\r\n",
        "        \r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        # read in the raw data, and explode the \"value\" and \"details\" arrays\r\n",
        "        df = spark.readStream.format('json').load(self.stage1np_graphapi_m365 + '/*/*.json', header='true')\r\n",
        "        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
        "        df = df.withColumn('reportPeriod', F.explode(F.col('details').reportPeriod)) \\\r\n",
        "                        .withColumn('mobile', F.explode(F.col('details').mobile)) \\\r\n",
        "                        .withColumn('web', F.explode(F.col('details').web)) \\\r\n",
        "                        .withColumn('mac', F.explode(F.col('details').mac)) \\\r\n",
        "                        .withColumn('windows', F.explode(F.col('details').windows)) \\\r\n",
        "                        .withColumn('excel', F.explode(F.col('details').excel)) \\\r\n",
        "                        .withColumn('excelMobile', F.explode(F.col('details').excelMobile)) \\\r\n",
        "                        .withColumn('excelWeb', F.explode(F.col('details').excelWeb)) \\\r\n",
        "                        .withColumn('excelMac', F.explode(F.col('details').excelMac)) \\\r\n",
        "                        .withColumn('excelWindows', F.explode(F.col('details').excelWindows)) \\\r\n",
        "                        .withColumn('oneNote', F.explode(F.col('details').oneNote)) \\\r\n",
        "                        .withColumn('oneNoteMobile', F.explode(F.col('details').oneNoteMobile)) \\\r\n",
        "                        .withColumn('oneNoteWeb', F.explode(F.col('details').oneNoteWeb)) \\\r\n",
        "                        .withColumn('oneNoteMac', F.explode(F.col('details').oneNoteMac)) \\\r\n",
        "                        .withColumn('oneNoteWindows', F.explode(F.col('details').oneNoteWindows)) \\\r\n",
        "                        .withColumn('outlook', F.explode(F.col('details').outlook)) \\\r\n",
        "                        .withColumn('outlookMobile', F.explode(F.col('details').outlookMobile)) \\\r\n",
        "                        .withColumn('outlookWeb', F.explode(F.col('details').outlookWeb)) \\\r\n",
        "                        .withColumn('outlookMac', F.explode(F.col('details').outlookMac)) \\\r\n",
        "                        .withColumn('outlookWindows', F.explode(F.col('details').outlookWindows)) \\\r\n",
        "                        .withColumn('powerPoint', F.explode(F.col('details').powerPoint)) \\\r\n",
        "                        .withColumn('powerPointMobile', F.explode(F.col('details').powerPointMobile)) \\\r\n",
        "                        .withColumn('powerPointWeb', F.explode(F.col('details').powerPointWeb)) \\\r\n",
        "                        .withColumn('powerPointMac', F.explode(F.col('details').powerPointMac)) \\\r\n",
        "                        .withColumn('powerPointWindows', F.explode(F.col('details').powerPointWindows)) \\\r\n",
        "                        .withColumn('teams', F.explode(F.col('details').teams)) \\\r\n",
        "                        .withColumn('teamsMobile', F.explode(F.col('details').teamsMobile)) \\\r\n",
        "                        .withColumn('teamsWeb', F.explode(F.col('details').teamsWeb)) \\\r\n",
        "                        .withColumn('teamsMac', F.explode(F.col('details').teamsMac)) \\\r\n",
        "                        .withColumn('teamsWindows', F.explode(F.col('details').teamsWindows)) \\\r\n",
        "                        .withColumn('word', F.explode(F.col('details').word)) \\\r\n",
        "                        .withColumn('wordMobile', F.explode(F.col('details').wordMobile)) \\\r\n",
        "                        .withColumn('wordWeb', F.explode(F.col('details').wordWeb)) \\\r\n",
        "                        .withColumn('wordMac', F.explode(F.col('details').wordMac)) \\\r\n",
        "                        .withColumn('wordWindows', F.explode(F.col('details').wordWindows)) \\\r\n",
        "                        .drop('details')\r\n",
        "        # change columns with dates to be of date types\r\n",
        "        df.select(F.col('reportRefreshDate'), F.to_date(F.col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
        "        df.select(F.col('lastActivityDate'), F.to_date(F.col('lastActivityDate'), 'yyyy-MM-dd'))\r\n",
        "        df.select(F.col('lastActivationDate'), F.to_date(F.col('lastActivationDate'), 'yyyy-MM-dd'))\r\n",
        "        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
        "        currentDate = datetime.datetime.now()\r\n",
        "        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
        "            # create a new column for partitioning the folder structure\r\n",
        "        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
        "        # use the m365_spark_schema for pseudonymization\r\n",
        "        m365_spark_schema = oea.to_spark_schema(self.schemas['m365'])\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['m365'])\r\n",
        "\r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_m365 + '/_checkpoints_p').partitionBy('ReportYearMonth')\r\n",
        "            query = query.start(self.stage2p + '/m365_app_user_detail_pseudo')\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query.lastProgress)\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_m365 + '/_checkpoints_np').partitionBy('ReportYearMonth')\r\n",
        "            query2 = query2.start(self.stage2np + '/m365_app_user_detail_lookup')\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query2.lastProgress)      \r\n",
        "\r\n",
        "    def _process_graphapi_teams_stage1_data(self):\r\n",
        "        \"\"\" Processes teams data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
        "        logger.info(\"Processing microsoft_graph teams data from: \" + self.stage1np_graphapi_teams)\r\n",
        "\r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        # read in the raw data, and explode the \"value\" and \"assignedProducts\" arrays \r\n",
        "        df = spark.readStream.format('json').load(self.stage1np_graphapi_teams + '/*/*.json', header='true')\r\n",
        "        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
        "        df = df.withColumn('assignedProducts', F.explode(F.col('assignedProducts')))\r\n",
        "            # convert duration to seconds only \r\n",
        "            # NOTE: The duration expression has changed and this will need to be modified to accommodate the new format\r\n",
        "        df = df.withColumn(\r\n",
        "            'screenShareDuration', \r\n",
        "            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
        "            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
        "            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
        "            ).withColumn(\r\n",
        "            'videoDuration', \r\n",
        "            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
        "            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
        "            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
        "            ).withColumn(\r\n",
        "            'audioDuration', \r\n",
        "            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
        "            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
        "            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
        "            )\r\n",
        "        # change columns with dates to be of date types\r\n",
        "        df.select(F.col('reportRefreshDate'), F.to_date(F.col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
        "        df.select(F.col('lastActivityDate'), F.to_date(F.col('lastActivityDate'), 'yyyy-MM-dd'))\r\n",
        "        # uncomment this code when using actual data, since this will be null in the test data\r\n",
        "        #df.select(F.col('deletedDate'), F.to_date(F.col('deletedDate'), 'yyyy-MM-dd'))\r\n",
        "        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
        "        currentDate = datetime.datetime.now()\r\n",
        "        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
        "            # create a new column for partitioning the folder structure\r\n",
        "        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
        "        # use the teams_spark_schema for pseudonymization\r\n",
        "        teams_spark_schema = oea.to_spark_schema(self.schemas['teams'])\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['teams'])\r\n",
        "\r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_teams + '/_checkpoints_p').partitionBy('ReportYearMonth')\r\n",
        "            query = query.start(self.stage2p + '/teams_activity_user_detail_pseudo')\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query.lastProgress)\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_teams + '/_checkpoints_np').partitionBy('ReportYearMonth')\r\n",
        "            query2 = query2.start(self.stage2np + '/teams_activity_user_detail_lookup')\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query2.lastProgress)   \r\n",
        "\r\n",
        "    def _process_graphapi_signInLogs_stage1_data(self):\r\n",
        "        \"\"\" Processes teams data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
        "        logger.info(\"Processing microsoft_graph teams data from: \" + self.stage1np_graphapi_signInLogs)\r\n",
        "\r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        # read in the raw data, and isolate only the needed columns\r\n",
        "        df = spark.readStream.format('json').load(self.stage1np_graphapi_signInLogs + '/*/*.json', header='true')\r\n",
        "        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
        "        df = df.select(F.col('id'), F.col('createdDateTime'), F.col('userPrincipalName'), F.col('ipAddress'))\r\n",
        "        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
        "        currentDate = datetime.datetime.now()\r\n",
        "        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
        "            # create a new column for partitioning the folder structure\r\n",
        "        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
        "        # use the signIn_spark_schema for pseudonymization\r\n",
        "        signIn_spark_schema = oea.to_spark_schema(self.schemas['sign_in_logs'])\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['sign_in_logs'])\r\n",
        "\r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_signInLogs + '/_checkpoints_p')\r\n",
        "            query = query.start(self.stage2p + '/sign_in_audit_logs_pseudo')\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query.lastProgress)\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_signInLogs + '/_checkpoints_np')\r\n",
        "            query2 = query2.start(self.stage2np + '/sign_in_audit_logs_lookup')\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query2.lastProgress)   \r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}