{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Graph Module - Refine\r\n",
        "\r\n",
        "This notebook demonstrates the utility of the OEA_py class notebook, and speeding up the process of refining/pseudonymizing the Graph data.\r\n",
        "\r\n",
        "The steps outlined below describe how this notebook is used to refine the Microsoft Graph module tables:\r\n",
        "\r\n",
        "- Set the workspace for where the tables are located. \r\n",
        "- 1 function is defined and used:\r\n",
        "   1. **refine_corrected**: almost identical to the ```oea.refine()``` function, except reads from ```stage2/Ingested_Corrected``` rather than ```stage2/Ingested```.\r\n",
        "   2. **refine_graph_dataset**: uses the Graph metadata_beta.csv and metadata_v1p0.csv to pseudonymize each table according to whether it is to be hashed, masked, or has no-operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "67",
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-12T18:24:01.4994364Z",
              "session_start_time": "2023-01-12T18:24:01.5350982Z",
              "execution_start_time": "2023-01-12T18:28:23.0162074Z",
              "execution_finish_time": "2023-01-12T18:28:23.1845508Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p3sm, 67, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "workspace = 'dev'\r\n",
        "testdataSet = 'hed'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "67",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-12T18:24:02.8407739Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-12T18:28:28.3421531Z",
              "execution_finish_time": "2023-01-12T18:28:28.3425007Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(, 67, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-01-12 18:28:27,631 - OEA - INFO - Now using workspace: dev\n2023-01-12 18:28:27,633 - OEA - INFO - OEA initialized.\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run OEA_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "67",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-12T18:24:24.5258906Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-12T18:28:28.5669226Z",
              "execution_finish_time": "2023-01-12T18:28:28.7286439Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p3sm, 67, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-01-12 18:28:28,547 - OEA - INFO - Now using workspace: dev\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace(workspace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "67",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-12T18:37:59.7514772Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-12T18:37:59.8782208Z",
              "execution_finish_time": "2023-01-12T18:38:00.0444188Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p3sm, 67, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2) this step refines the data through the use of metadata (this is where the pseudonymization of the data occurs).\r\n",
        "def refine_corrected(entity_path, metadata=None, primary_key='id'):\r\n",
        "    source_path = f'stage2/Ingested_Corrected/{entity_path}'\r\n",
        "    primary_key = oea.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
        "    path_dict = oea.parse_path(source_path)\r\n",
        "    sink_general_path = path_dict['entity_parent_path'].replace('Ingested_Corrected', 'Refined') + '/general/' + path_dict['entity']\r\n",
        "    sink_sensitive_path = path_dict['entity_parent_path'].replace('Ingested_Corrected', 'Refined') + '/sensitive/' + path_dict['entity'] + '_lookup'\r\n",
        "    if not metadata:\r\n",
        "        all_metadata = oea.get_metadata_from_path(path_dict['entity_parent_path'])\r\n",
        "        metadata = all_metadata[path_dict['entity']]\r\n",
        "\r\n",
        "    df_changes = oea.get_latest_changes(source_path, sink_general_path)\r\n",
        "    spark_schema = oea.to_spark_schema(metadata)\r\n",
        "    df_changes = oea.modify_schema(df_changes, spark_schema)        \r\n",
        "\r\n",
        "    if df_changes.count() > 0:\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df_changes, metadata)\r\n",
        "        oea.upsert(df_pseudo, sink_general_path, primary_key) # todo: remove this assumption that the primary key will always be hashed during pseduonymization\r\n",
        "        oea.upsert(df_lookup, sink_sensitive_path, primary_key)    \r\n",
        "        oea.add_to_lake_db(sink_general_path)\r\n",
        "        oea.add_to_lake_db(sink_sensitive_path)\r\n",
        "        logger.info(f'Processed {df_changes.count()} updated rows from {source_path} into stage2/Refined')\r\n",
        "    else:\r\n",
        "        logger.info(f'No updated rows in {source_path} to process.')\r\n",
        "    return df_changes.count()\r\n",
        "\r\n",
        "def refine_graph_dataset(tables_source):\r\n",
        "    items = oea.get_folders(tables_source)\r\n",
        "    for item in items: \r\n",
        "        table_path = tables_source +'/'+ item\r\n",
        "        if item == 'metadata.csv':\r\n",
        "            logger.info('ignore metadata processing, since this is not a table to be ingested')\r\n",
        "        else:\r\n",
        "            try:\r\n",
        "                if item == 'users':\r\n",
        "                    refine_corrected('graph_api/beta/' + item, metadata_beta[item], 'userPrincipalName_pseudonym')\r\n",
        "                elif item == 'm365_app_user_detail':\r\n",
        "                    refine_corrected('graph_api/beta/' + item, metadata_beta[item], 'm365Activity_pk_pseudonym')\r\n",
        "                elif item == 'teams_activity_user_detail':\r\n",
        "                    refine_corrected('graph_api/beta/' + item, metadata_beta[item], 'teamsActivity_pk_pseudonym')\r\n",
        "                elif item == 'meeting_attendance_report':\r\n",
        "                    refine_corrected('graph_api/v1.0/' + item, metadata_v1p0[item], 'meetingUserId_pk_pseudonym')\r\n",
        "                else:\r\n",
        "                    logger.info('No defined function for processing this Graph table.')\r\n",
        "            except AnalysisException as e:\r\n",
        "                # This means the table may have not been properly refined due to errors with the primary key not aligning with columns expected in the lookup table.\r\n",
        "                pass\r\n",
        "            \r\n",
        "            logger.info('Refined table: ' + item + ' from: ' + table_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "67",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-12T18:38:08.8920896Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-12T18:38:09.0314581Z",
              "execution_finish_time": "2023-01-12T18:39:05.2627222Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p3sm, 67, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-01-12 18:38:35,455 - OEA - INFO - Processed 512 updated rows from stage2/Ingested_Corrected/graph_api/beta/m365_app_user_detail into stage2/Refined\n2023-01-12 18:38:36,474 - OEA - INFO - Refined table: m365_app_user_detail from: stage2/Ingested_Corrected/graph_api/beta/m365_app_user_detail\n2023-01-12 18:38:45,103 - OEA - INFO - Processed 497 updated rows from stage2/Ingested_Corrected/graph_api/beta/teams_activity_user_detail into stage2/Refined\n2023-01-12 18:38:45,607 - OEA - INFO - Refined table: teams_activity_user_detail from: stage2/Ingested_Corrected/graph_api/beta/teams_activity_user_detail\n2023-01-12 18:38:54,621 - OEA - INFO - Processed 600 updated rows from stage2/Ingested_Corrected/graph_api/beta/users into stage2/Refined\n2023-01-12 18:38:55,025 - OEA - INFO - Refined table: users from: stage2/Ingested_Corrected/graph_api/beta/users\n2023-01-12 18:39:04,064 - OEA - INFO - Processed 10170 updated rows from stage2/Ingested_Corrected/graph_api/v1.0/meeting_attendance_report into stage2/Refined\n2023-01-12 18:39:04,464 - OEA - INFO - Refined table: meeting_attendance_report from: stage2/Ingested_Corrected/graph_api/v1.0/meeting_attendance_report\n2023-01-12 18:39:04,465 - OEA - INFO - Finished refining Graph HEd dataset\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "metadata_beta = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Graph/test_data/metadata_beta.csv')\r\n",
        "metadata_v1p0 = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Graph/test_data/metadata_v1p0.csv')\r\n",
        "if testdataSet == 'k12':\r\n",
        "    refine_graph_dataset('stage2/Ingested_Corrected/graph_api/beta')\r\n",
        "    logger.info('Finished refining Graph K-12 dataset')\r\n",
        "elif testdataSet == 'hed':\r\n",
        "    refine_graph_dataset('stage2/Ingested_Corrected/graph_api/beta')\r\n",
        "    refine_graph_dataset('stage2/Ingested_Corrected/graph_api/v1.0')\r\n",
        "    logger.info('Finished refining Graph HEd dataset')\r\n",
        "else:\r\n",
        "    logger.info('Unrecognized testdataSet - please choose either k12 or hed.')"
      ]
    }
  ],
  "metadata": {
    "description": "Used for \"3_refine_graph\" pipeline.",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}