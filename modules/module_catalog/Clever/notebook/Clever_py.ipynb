{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clever_py\r\n",
        "\r\n",
        "This OEA Clever Module python class provides:\r\n",
        "- Data schema definitions\r\n",
        "- Data pseudonomization settings\r\n",
        "- Data processing for Stage 1np data to Stage 2p and 2np\r\n",
        "\r\n",
        "__NOTE:__ This notebook currently only processes the Student Clever data (rather than students, teachers, and staff data)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\r\n",
        "\r\n",
        "class Clever(BaseOEAModule):\r\n",
        "    def __init__(self, source_folder='clever'):\r\n",
        "        BaseOEAModule.__init__(self, source_folder)\r\n",
        "\r\n",
        "        \"\"\"Data schema definitions and pseudonomization settings.\"\"\"\r\n",
        "\r\n",
        "        self.schemas['daily_participation'] = [['date', 'date', 'no-op'],\r\n",
        "                                            ['sis_id', 'string', 'hash'],\r\n",
        "                                            ['clever_user_id', 'string', 'hash'],\r\n",
        "                                            ['clever_school_id', 'string', 'no-op'],\r\n",
        "                                            ['school_name', 'string', 'no-op'],\r\n",
        "                                            ['active', 'boolean', 'no-op'],\r\n",
        "                                            ['num_logins', 'integer', 'no-op'],\r\n",
        "                                            ['num_resources_accessed', 'integer', 'no-op']]\r\n",
        "        self.schemas['resource_usage'] = [['date', 'date', 'no-op'], \r\n",
        "                                            ['sis_id', 'string', 'hash'],\r\n",
        "                                            ['clever_user_id', 'string', 'hash'],\r\n",
        "                                            ['clever_school_id', 'string', 'no-op'],\r\n",
        "                                            ['school_name', 'string', 'no-op'],\r\n",
        "                                            ['resource_type', 'string', 'no-op'],\r\n",
        "                                            ['resource_name', 'string', 'no-op'],\r\n",
        "                                            ['resource_id', 'string', 'no-op'],\r\n",
        "                                            ['num_access', 'integer', 'no-op']]\r\n",
        "\r\n",
        "    def ingest(self):\r\n",
        "        \"\"\"Processes clever data from stage1 into stage2 using structured streaming within the defined functions below.\"\"\"\r\n",
        "        logger.info(\"Processing clever data from: \" + self.stage1np)\r\n",
        "\r\n",
        "        items = mssparkutils.fs.ls(self.stage1np)\r\n",
        "        for item in items:\r\n",
        "            if item.name == \"daily-participation\":\r\n",
        "                self._process_clever_stage1_data(table_name='daily-participation', folder='/*/*', partition_column='date')\r\n",
        "            elif item.name == \"resource-usage\":\r\n",
        "                self._process_clever_stage1_data(table_name='resource-usage', folder='/*/*', partition_column='date')\r\n",
        "            else:\r\n",
        "                logger.info(\"No defined function for processing this clever data\")\r\n",
        "        \r\n",
        "        logger.info(\"Finished processing clever data from stage 1 to stage 2\")\r\n",
        "\r\n",
        "    def _process_clever_stage1_data(self, table_name=None, folder=None, partition_column=None):\r\n",
        "        \"\"\" Processes any clever data table from stage1 to stage2 using structured streaming.\r\n",
        "            NOTE: for the folder input, the second folder name can be changed to be either Student, Teacher, or Staff. \r\n",
        "            This currently processes all data in stage1, but the test data only contains Student. Update this as needed.\"\"\"\r\n",
        "        \r\n",
        "        # change new table name to match OEA conventions\r\n",
        "        new_table_name = table_name.replace('-', '_')\r\n",
        "\r\n",
        "        source_path = f'{self.stage1np}/{table_name}'\r\n",
        "        p_destination_path = f'{self.stage2p}/{new_table_name}_pseudo'\r\n",
        "        np_destination_path = f'{self.stage2np}/{new_table_name}_lookup'\r\n",
        "\r\n",
        "        logger.info(f'Processing incremental data from: {source_path} and writing out to: {p_destination_path}')\r\n",
        "        \r\n",
        "        clever_spark_schema = oea.to_spark_schema(self.schemas[new_table_name])\r\n",
        "        # read in the raw data using structured streaming\r\n",
        "        df = spark.readStream.csv(source_path + folder + '/*.csv', header='true', schema=clever_spark_schema)\r\n",
        "\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas[new_table_name])\r\n",
        "\r\n",
        "        # write out the resulting, final tables \r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            df_pseudo = df_pseudo.withColumn('year', F.year(F.col(partition_column))).withColumn('month', F.month(F.col(partition_column)))\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path +  '/_checkpoints_p').partitionBy('year', 'month')\r\n",
        "            query = query.start(p_destination_path)\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints_np')\r\n",
        "            query2 = query2.start(np_destination_path)\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}