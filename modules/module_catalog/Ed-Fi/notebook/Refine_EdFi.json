{
    "name": "Refine_EdFi",
    "properties": {
        "folder": {
            "name": "Modules/Ed-Fi"
        },
        "nbformat": 4,
        "nbformat_minor": 2,

        "sessionProperties": {
            "driverMemory": "28g",
            "driverCores": 4,
            "executorMemory": "28g",
            "executorCores": 4,
            "numExecutors": 2,
            "runAsWorkspaceSystemIdentity": false,
            "conf": {
                "spark.dynamicAllocation.enabled": "false",
                "spark.dynamicAllocation.minExecutors": "2",
                "spark.dynamicAllocation.maxExecutors": "2",
                "spark.autotune.trackingId": "cce40479-bb51-43e5-a975-0b94a0843723"
            }
        },
        "metadata": {
            "saveOutput": true,
            "synapse_widget": {
                "version": "0.1"
            },
            "enableDebugMode": false,
            "kernelspec": {
                "name": "synapse_pyspark",
                "display_name": "Synapse PySpark"
            },
            "language_info": {
                "name": "python"
            },
            "sessionKeepAliveTimeout": 30
        },
        "cells": [
            {
                "cell_type": "code",
                "source": [
                    "%run /OEA_0p7_py"
                ],
                "outputs": [],
                "execution_count": 48
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "%run /OpenAPIUtil_py"
                ],
                "outputs": [],
                "execution_count": 59
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Pass the below parameters from pipeline. \r\n",
                    "directory = 'Ed-Fi'\r\n",
                    "api_version = '5.2'\r\n",
                    "metadata_url = 'https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Ed-Fi/docs/edfi_oea_metadata.csv'\r\n",
                    "swagger_url = 'https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev//modules/module_catalog/Ed-Fi/docs/edfi_swagger.json'\r\n",
                    "# TODO: swagger_url = 'https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/123/2022/resources/swagger.json'\r\n",
                    "# KeyError exception because the 'x-Ed-Fi-explode' is not part of the standard swager.json\r\n",
                    "# We should remove all references to 'x-Ed-Fi-explode' from these notebooks since we are now dynamicallly exploding arrays based on swagger datatypes\r\n",
                    "\r\n",
                    "oea = OEA()\r\n",
                    "oea_metadatas = oea.get_metadata_from_url(metadata_url)\r\n",
                    "primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']\r\n",
                    "\r\n",
                    "# TODO: Use the swagger file available from the Ed-Fi API landing page, instead of hardcoding it.\r\n",
                    "# For example, https://api.edgraph.dev/edfi/v5.2/saas is the openApiMetadata endpoint will help fetch the descriptors and resources swagger.json\r\n",
                    "# The base path of the api can be passed as a parameter to this notebook instead and assigned to swagger_url variable\r\n",
                    "# This will also help get latest version of the swagger.json based on the Ed-Fi version and it will contain a list of all endpoints \r\n",
                    "# and entity definition, including any \"extensions\" or custommizations\r\n",
                    "# For example, \r\n",
                    "# - Descriptors: https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/123/2022/resources/swagger.JSON\r\n",
                    "# - Resources:   https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/123/2022/descriptors/swagger.JSON\r\n",
                    "schema_gen = OpenAPIUtil(swagger_url)\r\n",
                    "schemas = schema_gen.create_spark_schemas()\r\n",
                    "\r\n",
                    "stage2_ingested = oea.to_url(f'stage2/Ingested/{directory}/v{api_version}')\r\n",
                    "stage2_refined = oea.to_url(f'stage2/Refined/{directory}/v{api_version}')"
                ],
                "outputs": [],
                "execution_count": 60
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "def get_descriptor_schema(descriptor):\r\n",
                    "    fields = []\r\n",
                    "    fields.append(StructField('_etag',LongType(), True))\r\n",
                    "    fields.append(StructField(f\"{descriptor[:-1]}Id\", IntegerType(), True))\r\n",
                    "    fields.append(StructField('codeValue',StringType(), True))\r\n",
                    "    fields.append(StructField('description',StringType(), True))\r\n",
                    "    fields.append(StructField('id',StringType(), True))\r\n",
                    "    fields.append(StructField('namespace',StringType(), True))\r\n",
                    "    fields.append(StructField('shortDescription',StringType(), True))\r\n",
                    "    return StructType(fields)\r\n",
                    "\r\n",
                    "def get_descriptor_metadata(descriptor):\r\n",
                    "    return [['_etag', 'long', 'no-op'],\r\n",
                    "            [f\"{descriptor[:-1]}Id\", 'integer', 'hash'],\r\n",
                    "            ['codeValue','string', 'no-op'],\r\n",
                    "            ['description','string', 'no-op'],\r\n",
                    "            ['id','string', 'no-op'],\r\n",
                    "            ['namespace','string', 'no-op'],\r\n",
                    "            ['shortDescription','string', 'no-op']]"
                ],
                "outputs": [],
                "execution_count": 61
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "import copy\r\n",
                    "from pyspark.sql.functions import when\r\n",
                    "\r\n",
                    "def has_column(df, col):\r\n",
                    "    try:\r\n",
                    "        df[col]\r\n",
                    "        return True\r\n",
                    "    except AnalysisException:\r\n",
                    "        return False\r\n",
                    "\r\n",
                    "def modify_descriptor_value(df, col_name):\r\n",
                    "    if col_name in df.columns:\r\n",
                    "        # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\r\n",
                    "        df = df.withColumn(f\"{col_name}LakeId\", f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.regexp_replace(col_name, '#', '_')))\r\n",
                    "        df = df.drop(col_name)\r\n",
                    "    else:\r\n",
                    "        df = df.withColumn(f\"{col_name}LakeId\", f.lit(None).cast(\"String\"))\r\n",
                    "\r\n",
                    "    return df\r\n",
                    "\r\n",
                    "def flatten_reference_col(df, target_col):\r\n",
                    "    col_prefix = target_col.name.replace('Reference', '')\r\n",
                    "    df = df.withColumn(f\"{col_prefix}LakeId\", when(f.col(target_col.name).isNotNull(), f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.split(f.col(f'{target_col.name}.link.href'), '/').getItem(3))))\r\n",
                    "    df = df.drop(target_col.name)\r\n",
                    "    return df\r\n",
                    "\r\n",
                    "def modify_references_and_descriptors(df, target_col):\r\n",
                    "    for ref_col in [x for x in df.columns if re.search('Reference$', x) is not None]:\r\n",
                    "        df = flatten_reference_col(df, target_col.dataType.elementType[ref_col])\r\n",
                    "    for desc_col in [x for x in df.columns if re.search('Descriptor$', x) is not None]:\r\n",
                    "        df = modify_descriptor_value(df, desc_col)\r\n",
                    "    return df\r\n",
                    "\r\n",
                    "def explode_arrays(df, target_col, schema_name, table_name):\r\n",
                    "    cols = ['lakeId', 'DistrictId', 'SchoolYear']\r\n",
                    "    child_df = df.select(cols + [target_col.name])\r\n",
                    "    child_df = child_df.withColumn(\"exploded\", f.explode(target_col.name)).drop(target_col.name).select(cols + ['exploded.*'])\r\n",
                    "\r\n",
                    "    # TODO: It looks like te {target_col.name}LakeId column is not addedd to the child entities\r\n",
                    "    #       We should use LakeId suffix when using the \"id\" column from the parent and HKey suffix when creating a Hash Key based on composite key columns\r\n",
                    "    identity_cols = [x.name for x in target_col.dataType.elementType.fields if 'x-Ed-Fi-isIdentity' in x.metadata].sort()\r\n",
                    "    if(identity_cols is not None and len(identity_cols) > 0):\r\n",
                    "        child_df = child_df.withColumn(f\"{target_col.name}LakeId\", f.concat(f.col('DistrictId'), f.lit('_'), f.col('SchoolYear'), f.lit('_'), *[f.concat(f.col(x), f.lit('_')) for x in identity_cols]))\r\n",
                    "    \r\n",
                    "    # IMPORTANT: We must modify Reference and Descriptor columns for child columns \"first\". \r\n",
                    "    # This must be done \"after\" the composite key from identity_cols has been created otherwise the columns are renamed and will not be found by identity_cols.\r\n",
                    "    # This must be done \"before\" the grand_child is exploded below\r\n",
                    "    child_df = modify_references_and_descriptors(child_df, target_col)\r\n",
                    "\r\n",
                    "    for array_sub_col in [x for x in target_col.dataType.elementType.fields if x.dataType.typeName() == 'array' ]:\r\n",
                    "        grand_child_df = child_df.withColumn('exploded', f.explode(array_sub_col.name)).select(child_df.columns + ['exploded.*']).drop(array_sub_col.name)\r\n",
                    "        \r\n",
                    "        # Modifying Reference and Descriptor columns for the grand_child array\r\n",
                    "        grand_child_df = modify_references_and_descriptors(grand_child_df, array_sub_col)\r\n",
                    "\r\n",
                    "        # TODO: Pseudonimize and Write to Sensitive folder for child arrays?\r\n",
                    "        grand_child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear')\\\r\n",
                    "                .save(f\"{stage2_refined}/General/{schema_name}/{table_name}_{target_col.name}_{array_sub_col.name}\")\r\n",
                    "\r\n",
                    "    # TODO: Pseudonimize and Write to Sensitive folder for child arrays?\r\n",
                    "    child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear')\\\r\n",
                    "        .save(f\"{stage2_refined}/General/{schema_name}/{table_name}_{target_col.name}\")\r\n",
                    "\r\n",
                    "    # Drop array column from parent entity\r\n",
                    "    df = df.drop(target_col.name)\r\n",
                    "    return df\r\n",
                    "\r\n",
                    "def transform(df, schema_name, table_name, parent_schema_name, parent_table_name):\r\n",
                    "    if re.search('Descriptors$', table_name) is None:\r\n",
                    "        # Use Deep Copy otherwise the schemas object also gets modified every time target_schema is modified\r\n",
                    "        target_schema = copy.deepcopy(schemas[table_name])\r\n",
                    "        # Add primary key\r\n",
                    "        if has_column(df, 'id'):\r\n",
                    "            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('id')).cast(\"String\"))\r\n",
                    "        else:\r\n",
                    "            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\r\n",
                    "    else:\r\n",
                    "        target_schema = get_descriptor_schema(table_name)\r\n",
                    "        # Add primary key\r\n",
                    "        if has_column(df, 'namespace') and has_column(df, 'codeValue'):\r\n",
                    "            # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\r\n",
                    "            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('namespace'), f.col('codeValue')).cast(\"String\"))\r\n",
                    "        else:\r\n",
                    "            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\r\n",
                    "\r\n",
                    "    target_schema = target_schema.add(StructField('DistrictId', StringType()))\\\r\n",
                    "                                 .add(StructField('SchoolYear', StringType()))\\\r\n",
                    "                                 .add(StructField('LastModifiedDate', TimestampType()))\r\n",
                    "\r\n",
                    "    for col_name in target_schema.fieldNames():\r\n",
                    "        target_col = target_schema[col_name]\r\n",
                    "        # If Primitive datatype, i.e String, Bool, Integer, etc.abs\r\n",
                    "        # Note: Descriptor is a String therefore is a Primitive datatype\r\n",
                    "        if target_col.dataType.typeName() in primitive_datatypes:\r\n",
                    "            # If it is a Descriptor\r\n",
                    "            if re.search('Descriptor$', col_name) is not None:\r\n",
                    "                df = modify_descriptor_value(df, col_name)\r\n",
                    "            else:\r\n",
                    "                if col_name in df.columns:\r\n",
                    "                    # Casting columns to primitive data types\r\n",
                    "                    df = df.withColumn(col_name, f.col(col_name).cast(target_col.dataType))\r\n",
                    "                else:\r\n",
                    "                    # If Column not present in dataframe, add column with None values.\r\n",
                    "                    df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
                    "        # If Complex datatype, i.e. Object, Array\r\n",
                    "        else:\r\n",
                    "            if col_name not in df.columns:\r\n",
                    "                df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
                    "            else:\r\n",
                    "                # Generate JSON column as a Complex Type\r\n",
                    "                df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name))) \\\r\n",
                    "                    .withColumn(col_name, f.from_json(f.col(f\"{col_name}_json\"), target_col.dataType)) \\\r\n",
                    "                    .drop(f\"{col_name}_json\")\r\n",
                    "            \r\n",
                    "            # Modify the links with surrogate keys\r\n",
                    "            if re.search('Reference$', col_name) is not None:\r\n",
                    "                df = flatten_reference_col(df, target_col)\r\n",
                    "    \r\n",
                    "            if target_col.dataType.typeName() == 'array':\r\n",
                    "                df = explode_arrays(df, target_col, schema_name, table_name)\r\n",
                    "        \r\n",
                    "    return df\r\n",
                    "\r\n",
                    "#df = spark.read.format('delta').load(f\"{stage2_ingested}/ed-fi/graduationPlans\")\r\n",
                    "#df = transform(df, \"ed-fi\", \"graduationPlans\", None, None)"
                ],
                "outputs": [],
                "execution_count": 62
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "for schema_name in [x.name for x in mssparkutils.fs.ls(stage2_ingested) if x.isDir]:\r\n",
                    "    print(f\"Processing schema: {schema_name}\")\r\n",
                    "    \r\n",
                    "    for table_name in [y.name for y in mssparkutils.fs.ls(f\"{stage2_ingested}/{schema_name}\") if y.isDir]:\r\n",
                    "        print(f\"Processing schema/table: {schema_name}/{table_name}\")\r\n",
                    "\r\n",
                    "        # 1. Read Delta table from Ingested Folder.\r\n",
                    "\r\n",
                    "        # Process each file even when it is empty. The tables will be created using on target_schema and will be available for query in SQL.\r\n",
                    "        df = spark.read.format('delta').load(f\"{stage2_ingested}/{schema_name}/{table_name}\")\r\n",
                    "\r\n",
                    "        # 2. Transformation step\r\n",
                    "        try:\r\n",
                    "            df = transform(df, schema_name, table_name, None, None)\r\n",
                    "        except:\r\n",
                    "            print(f\"Error while Transforming {schema_name}/{table_name}\")\r\n",
                    "\r\n",
                    "        # 3. Pseudonymize the data using metadata.\r\n",
                    "        if(re.search('Descriptors$', table_name) is None):\r\n",
                    "            # Use Deep Copy otherwise the schemas object also gets modified every time oea_metadatas is modified\r\n",
                    "            oea_metadata = copy.deepcopy(oea_metadatas[table_name])\r\n",
                    "        else:\r\n",
                    "            oea_metadata = get_descriptor_metadata(table_name)\r\n",
                    "\r\n",
                    "        oea_metadata += [\r\n",
                    "                            ['DistrictId', 'string', 'partition-by'],\r\n",
                    "                            ['SchoolYear', 'string', 'partition-by'],\r\n",
                    "                            ['LastModifiedDate', 'timestamp', 'no-op']\r\n",
                    "                        ]\r\n",
                    "\r\n",
                    "        try:\r\n",
                    "            df_pseudo, df_lookup = oea.pseudonymize(df, oea_metadata)\r\n",
                    "        except:\r\n",
                    "            print(f\"Error while Pseudonymizing {schema_name}/{table_name}\")\r\n",
                    "\r\n",
                    "        # 4. Write to Refined folder (even when file is empty)\r\n",
                    "        df_pseudo.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear').save(f\"{stage2_refined}/General/{schema_name}/{table_name}\")\r\n",
                    "        #if(len(df_lookup.columns) > 2):\r\n",
                    "        df_lookup.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear').save(f\"{stage2_refined}/Sensitive/{schema_name}/{table_name}\")\r\n",
                    ""
                ],
                "outputs": [],
                "execution_count": 63
            }
        ]
    }
}