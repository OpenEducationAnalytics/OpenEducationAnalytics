{
	"name": "oea_utils",
	"properties": {
		"folder": {
			"name": "oea_framework"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Microsoft Spark Utilities\r\n",
					"[Intro to Microsoft Spark Utilities](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"from notebookutils import mssparkutils\r\n",
					"# to list out all of the available commands\r\n",
					"mssparkutils.fs.help()\r\n",
					"\r\n",
					"# an example of deleting a directory (and everything within it)\r\n",
					"mssparkutils.fs.rm('abfss://stage1@stoeacisd3gg1.dfs.core.windows.net/tutorial_01', True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# delete from M365 stage2, move files from \"processed\" back into \"inbound\" in stage1\r\n",
					"from notebookutils import mssparkutils\r\n",
					"mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
					"mssparkutils.fs.mv('abfss://stage1@stoeacisd3gg1.dfs.core.windows.net/M365/processed/roster/2021-06-15T04-04-12', stage1 + '/M365/inbound/roster', True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Logging example. More info at: https://github.com/balakreshnan/Samples2021/blob/main/Synapseworkspace/opencensuslog.md#azure-synapse-spark-logs-runtime-errors-to-application-insights\r\n",
					"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
					"\r\n",
					"logger = logging.getLogger(__name__)\r\n",
					"logger.setLevel(logging.INFO) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
					"logger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=<insert instrumentation key here>'))\r\n",
					"\r\n",
					"logger.info(\"OEA info msg\")\r\n",
					"logger.debug(\"OEA: debug msg\")\r\n",
					"logger.debug(\"OEA: warning msg\")\r\n",
					"logger.error(\"OEA: error msg\")\r\n",
					"logger.exception(\"OEA: exception msg\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Check on existence of a folder in the data lake.\r\n",
					"\r\n",
					"from notebookutils import mssparkutils\r\n",
					"\r\n",
					"def folderExists(path, foldername):\r\n",
					"    items = mssparkutils.fs.ls(path)\r\n",
					"    tableExists = False\r\n",
					"    for item in items:\r\n",
					"        if item.name == foldername:\r\n",
					"            tableExists = True\r\n",
					"    return tableExists\r\n",
					"\r\n",
					"#print(folderExists(stage1 + '/tutorial_01/activity/2021-06-10', 'TechActivity'))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Create an empty delta table if one doesn't yet exist\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
					"from delta.tables import *\r\n",
					"\r\n",
					"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
					"\r\n",
					"if not DeltaTable.isDeltaTable(spark, stage2 + '/tutorial_01/TechActivity'):\r\n",
					"    emptyDF = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\r\n",
					"    emptyDF.write.format('delta').mode('overwrite').save(stage2 + '/tutorial_01/TechActivity')"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Reset M365 data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Delete stage2/M365 if it exists\r\n",
					"files = mssparkutils.fs.ls(stage2)\r\n",
					"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
					"for file in files:\r\n",
					"    print(file.name)\r\n",
					"    if file.name == 'M365':\r\n",
					"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
					"\r\n",
					"# Move roster data back in to \"inbound\" folder\r\n",
					"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
					"for file in files:\r\n",
					"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
					"\r\n",
					"    "
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Structured Streaming\r\n",
					"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\r\n",
					"\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
					"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
					"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
					"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
					"#https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink\r\n",
					"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').start(dest_path)\r\n",
					"\r\n",
					"query.stop()\r\n",
					"print(query.id)\r\n",
					"print(query.explain())\r\n",
					"print(spark.streams.active)"
				],
				"execution_count": null
			}
		]
	}
}