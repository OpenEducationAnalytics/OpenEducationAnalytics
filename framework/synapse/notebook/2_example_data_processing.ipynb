{
	"name": "2_example_data_processing",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark3p2sm",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "14be358e-ebf8-49c2-971c-a39ecf3710ba"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-cisd31101e/providers/Microsoft.Synapse/workspaces/syn-oea-cisd31101e/bigDataPools/spark3p2sm",
				"name": "spark3p2sm",
				"type": "Spark",
				"endpoint": "https://syn-oea-cisd31101e.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p2sm",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Example data processing\r\n",
					"This example demonstrates how a data engineer utilizes OEA to work with data from a new data source."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run OEA_py"
				],
				"execution_count": 268
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
					"# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
					"# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
					"oea.set_workspace('sam')"
				],
				"execution_count": 269
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 2) Now land a batch data file into stage1 of the data lake.\r\n",
					"# In this example we pull a test csv data file from github and it is landed in oea/sandboxes/sam/stage1/Transactional/contoso/v0.1/students/delta_batch_data/rundate=<utc datetime>\r\n",
					"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day1/students/part1.csv')\r\n",
					"oea.land_delta_batch_data(data, 'contoso/v0.1/students', 'students.csv')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# 3) You can verify that the data is in stage1 by reading it into a dataframe. Note that a \"rundate\" column has been added - representing the datetime that the batch data was landed in the data lake.\r\n",
					"df = oea.load_csv(f'stage1/Transactional/contoso/v0.1/students')\r\n",
					"display(df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 4) The next step is to ingest the batch data into stage2\r\n",
					"# Note that when you run this the first time, you'll see an info message like \"Number of new inbound rows processed: 2\",\r\n",
					"# but if you run it a second time, the number of inbound rows processed will be 0 because the ingestion uses spark structured streaming to keep track of what data has already been processed.\r\n",
					"oea.ingest(f'contoso/v0.1/students', 'SIS ID')"
				],
				"execution_count": 252
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# 5) When data is ingested into stage2 of the data lake, OEA creates a lake db (which is a logical db that points to the data in the data lake).\r\n",
					"# In this example, since you are working in the 'sam' workspace, the lake db created is called 'ldb_sam_s2i_contoso_v0p1' (if you click on Data in the left nav, you'll see the db listed under 'Lake database' )\r\n",
					"df = spark.sql(\"select * from ldb_sam_s2i_contoso_v0p1.students\")\r\n",
					"display(df)\r\n",
					"#df.printSchema()"
				],
				"execution_count": 253
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 6) Now let's land some additional inbound batch data - with new and modified rows.\r\n",
					"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day2/students/part1.csv')\r\n",
					"oea.land_delta_batch_data(data, 'contoso/v0.1/students', 'students.csv')"
				],
				"execution_count": 254
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 7) Ingest this latest batch of data.\r\n",
					"# Note that you don't have to specify what batch of data to process; OEA uses spark structured streaming to determine what files are new.\r\n",
					"oea.ingest(f'contoso/v0.1/students', 'SIS ID')"
				],
				"execution_count": 274
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# 8) Now verify that the batch data was ingested and correctly merged with the previous data\r\n",
					"\r\n",
					"# You can load the ingested data into a dataframe directly like this...\r\n",
					"df = oea.load('stage2/Ingested/contoso/v0.1/students')\r\n",
					"display(df)\r\n",
					"\r\n",
					"# ...or you can use the automatically created \"Lake database\" like this:\r\n",
					"df = spark.sql(\"select * from ldb_sam_s2i_contoso_v0p1.students\")\r\n",
					"display(df)\r\n",
					"# with either approach, you're querying the same data - it's the data stored at oea/sandboxes/sam/stage2/Ingested/contoso/v0.1/students in your data lake"
				],
				"execution_count": 256
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# 9) After ingesting data, the next step is to refine the data through the use of metadata\r\n",
					"metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/metadata.csv')\r\n",
					"oea.refine('contoso/v0.1/students', metadata['students'], 'SIS ID')"
				],
				"execution_count": 275
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# 11) Now you can query the refined data tables in the lake db\r\n",
					"df = spark.sql(\"select * from ldb_sam_s2r_contoso_v0p1.students\")\r\n",
					"display(df)\r\n",
					"df = spark.sql(\"select * from ldb_sam_s2r_contoso_v0p1.students_lookup\")\r\n",
					"display(df)\r\n",
					"# You can use the \"lookup\" table for joins (people with restricted access won't be able to perform this query because they won't have access to data in the \"sensitive\" folder in the data lake)\r\n",
					"df = spark.sql(\"select sl.Username, s.Grade from ldb_sam_s2r_contoso_v0p1.students_lookup sl, ldb_sam_s2r_contoso_v0p1.students s where sl.SIS_ID_pseudonym = s.SIS_ID_pseudonym\")\r\n",
					"display(df)"
				],
				"execution_count": 276
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 11) Land, ingest, and refine additional data sets.\r\n",
					"# These data sets demonstrate the 2 other types of batch data - additive and snapshot.\r\n",
					"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day1/studentattendance/part1.csv')\r\n",
					"oea.land_additive_batch_data(data, 'contoso/v0.1/studentattendance', 'part1.csv')\r\n",
					"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day2/studentattendance/part1.csv')\r\n",
					"oea.land_additive_batch_data(data, 'contoso/v0.1/studentattendance', 'part1.csv')\r\n",
					"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day1/studentsectionmark/part1.csv')\r\n",
					"oea.land_snapshot_batch_data(data, 'contoso/v0.1/studentsectionmark', 'part1.csv')\r\n",
					"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day2/studentsectionmark/part1.csv')\r\n",
					"oea.land_snapshot_batch_data(data, 'contoso/v0.1/studentsectionmark', 'part1.csv')\r\n",
					"\r\n",
					"oea.ingest(f'contoso/v0.1/studentattendance', 'id')\r\n",
					"oea.ingest(f'contoso/v0.1/studentsectionmark', 'id')\r\n",
					"\r\n",
					"metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/metadata.csv')\r\n",
					"oea.refine('contoso/v0.1/studentattendance', metadata['studentattendance'], 'id')\r\n",
					"oea.refine('contoso/v0.1/studentsectionmark', metadata['studentsectionmark'], 'id')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# 12) Reset this example\r\n",
					"oea.rm_if_exists('stage1/Transactional/contoso')\r\n",
					"oea.rm_if_exists('stage2/Ingested/contoso')\r\n",
					"oea.rm_if_exists('stage2/Refined/contoso')\r\n",
					"oea.drop_lake_db('ldb_sam_s2i_contoso_v0p1')\r\n",
					"oea.drop_lake_db('ldb_sam_s2r_contoso_v0p1')"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Appendix"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# You can list the contents of a folder in the data lake like this:\r\n",
					"print(oea.ls('stage1/Transactional/contoso/v0.1/students/delta_batch_data'))\r\n",
					"print(oea.ls('stage2/Ingested/contoso/v0.1/students'))"
				],
				"execution_count": 76
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"dtbl = DeltaTable.forPath(spark, oea.to_url('stage2/Refined/contoso/v0.1/sensitive/students_lookup'))\r\n",
					"display(dtbl.toDF())\r\n",
					"#dtbl.delete(\"rundate > '2022-11-04T14:39:51'\")"
				],
				"execution_count": null
			}
		]
	}
}