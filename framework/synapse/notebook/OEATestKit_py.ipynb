{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%run /OEA_py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oea.set_workspace(workspace)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as f\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class OEATestKit:\r\n",
        "    #---------------------------- Dataframe Generators ----------------------------\r\n",
        "    \"\"\"\r\n",
        "    Gets a dataframe from a raw data file\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "        primary_key(string): the primary key for the entity\r\n",
        "        entity_path (string): the path to the entity\r\n",
        "        options (object): options about how to infer the schema\r\n",
        "    \"\"\"\r\n",
        "    def get_raw_dataframe(self, primary_key, entity_path, options):\r\n",
        "        primary_key = oea.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
        "        ingested_path = f'stage2/Ingested/{entity_path}'\r\n",
        "        raw_path = f'stage1/Transactional/{entity_path}'\r\n",
        "        batch_type, source_data_format = oea.get_batch_info(raw_path)\r\n",
        "        source_url = oea.to_url(f'{raw_path}/{batch_type}_batch_data')\r\n",
        "\r\n",
        "        if batch_type == 'snapshot'or batch_type=='additive': source_url = f'{source_url}/{oea.get_latest_folder(source_url)}'\r\n",
        "\r\n",
        "        if options == None: options = {}\r\n",
        "        options['format'] = source_data_format # eg, 'csv', 'json'\r\n",
        "        if source_data_format == 'csv' and (not 'header' in options or options['header'] == None): options['header'] = True  # default to expecting a header in csv files\r\n",
        "\r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        return spark.read.format('delta').load(oea.to_url(source_url), **options)\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Gets a dataframe from a lake database\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "        item (string): the name of the entity to retrieve from the lake database\r\n",
        "        stage (int): the number of the stage to get the dataframe from\r\n",
        "        type_id (string): The type identifier.  i for ingest or r for refine\r\n",
        "        collection: The name of the collection, aka the name of the parentmost folder\r\n",
        "        version (string): The version number\r\n",
        "    \"\"\"\r\n",
        "    def get_lake_dataframe(self,item, stage, type_id, collection, version, path_modifier=None):\r\n",
        "        version_delimiter = \"p\"\r\n",
        "        version_split = version.split(\".\")\r\n",
        "        namespace = f\"ldb_{oea.workspace}_s{stage}{type_id.lower()}_{collection.lower()}_v{version_delimiter.join(version_split)}\"\r\n",
        "        if path_modifier:\r\n",
        "            namespace += f'_{path_modifier}'\r\n",
        "        return spark.sql(f\"SELECT * FROM {namespace}.{item.lower()}\")\r\n",
        "\r\n",
        "    #------------------------------ Utility Methods --------------------------------\r\n",
        "    \"\"\"\r\n",
        "    This method checks if all of the rows in the first dataframe exist in the second dataframe, by\r\n",
        "    comparing the dataframes on the primary key.  Returns True if all the entities in df1 appear in df2\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "        df1 (Spark.Dataframe): Dataframe of the first set of data\r\n",
        "        df2 (Spark.Dataframe): Dataframe of the second set of data\r\n",
        "        primary_key (string | string[]): the primary key(s) used for comparison \r\n",
        "    \"\"\"\r\n",
        "    def is_subset(self, df1, df2, primary_key):\r\n",
        "        merged_df = pd.merge(df1.toPandas(), df2.toPandas(), on=primary_key, how=\"left\", indicator=\"exists\")\r\n",
        "        merged_df['exists'] = np.where(merged_df.exists == 'both', True, False)\r\n",
        "        return merged_df[merged_df.exists == False].shape[0] == 0\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    This method checks if the df has any duplicates using the primary_key as a comparison.  Returns True if there are duplicates\r\n",
        "    else it returns false\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "        df (Spark.Dataframe): the dataframe to test\r\n",
        "        primary_key (string | string[]): The primary key column(s)\r\n",
        "    \"\"\"\r\n",
        "    def has_duplicates(self, df, primary_key):\r\n",
        "        grouped_df = df.groupBy(primary_key).count()\r\n",
        "        return grouped_df.where(f.col('count') > 1).count() > 0\r\n",
        "    \r\n",
        "    #------------------------- Test Cases --------------------------------------\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Tests to confirm that the raw dataset is a subset of the lake database, thus ensuring all the entities in the raw data\r\n",
        "    exist in the lake database\r\n",
        "    \"\"\"\r\n",
        "    def test_raw_data_is_subset_of_lake(self, collection, version, item, primary_key, options, path_modifier=None):\r\n",
        "        #Get the lake database as a dataframe\r\n",
        "        lake_df = self.get_lake_dataframe(item, 2, \"i\", collection, version, path_modifier)\r\n",
        "\r\n",
        "        #get the raw data as a dataframe\r\n",
        "        entity_path = f'{collection}/v{version}/'\r\n",
        "        entity_path += f'{path_modifier}/{item}' if path_modifier else item\r\n",
        "        raw_df = self.get_raw_dataframe(primary_key, entity_path, options)\r\n",
        "\r\n",
        "        #check if the raw data exists in the lake database\r\n",
        "        assert self.is_subset(raw_df, lake_df, primary_key)\r\n",
        "    \r\n",
        "    def test_has_duplicates(self, collection, version, item, primary_key, path_modifier=None):\r\n",
        "        #Get the lake database as a dataframe\r\n",
        "        lake_df = self.get_lake_dataframe(item, 2, \"i\", collection, version, path_modifier)\r\n",
        "        assert self.has_duplicates(lake_df, primary_key) != True\r\n",
        "        \r\n",
        "oea_test_kit = OEATestKit()\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "91",
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-04-06T21:47:27.3721165Z",
              "session_start_time": "2023-04-06T21:47:27.4292412Z",
              "execution_start_time": "2023-04-06T21:48:40.9648496Z",
              "execution_finish_time": "2023-04-06T21:48:41.1375511Z",
              "spark_jobs": null,
              "parent_msg_id": "b468eb04-1df3-4a22-b633-9873d597f588"
            },
            "text/plain": "StatementMeta(spark3p2med, 91, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block (2641926224.py, line 84)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_14376/2641926224.py\"\u001b[0;36m, line \u001b[0;32m84\u001b[0m\n\u001b[0;31m    oea_test_kit = OEATestKit()\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}