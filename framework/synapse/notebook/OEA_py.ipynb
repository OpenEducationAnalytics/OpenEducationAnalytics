{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "####### OEA configuration #############\n",
        "oea_storage_account = 'yourstorageaccount'\n",
        "oea_keyvault = 'yourkeyvault'\n",
        "oea_timezone = 'US/Eastern'\n",
        "#######################################\n",
        "\n",
        "from delta.tables import DeltaTable\n",
        "from notebookutils import mssparkutils\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "import logging\n",
        "import pandas as pd\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import datetime\n",
        "import pytz\n",
        "import random\n",
        "import io\n",
        "import requests\n",
        "\n",
        "logger = logging.getLogger('OEA')\n",
        "\n",
        "class OEA:\n",
        "    \"\"\" OEA (Open Education Analytics) framework simplifies the process of working with large data sets within the context of a lakehouse architecture.\n",
        "        Definition of terms used throughout this codebase:\n",
        "        path - a complete or partial folder or file path (does not include details like scheme or domain name as found in a URL). Ex: contosos/v0.1/students\n",
        "        entity_path - a path that ends with a folder that contains entity data. Ex: contoso/v0.1/students\n",
        "        dataset_path - a path that ends with a folder that contains entity folders (entity parent folder). Ex: contoso/v0.1\n",
        "        url - includes scheme and domain name. Ex: abfss://stage1@storageaccount.dfs.core.windows.net/contoso/v0.1/students\n",
        "\n",
        "    \"\"\"\n",
        "    DELTA_BATCH_DATA = 'delta_batch_data'\n",
        "    ADDITIVE_BATCH_DATA = 'additive_batch_data'\n",
        "    SNAPSHOT_BATCH_DATA = 'snapshot_batch_data'\n",
        "\n",
        "    def __init__(self, workspace='dev', logging_level=logging.INFO, storage_account=None, keyvault=None, timezone=None):\n",
        "        self.keyvault_linked_service = 'LS_KeyVault'\n",
        "        self.salt_secret_name = 'oeaSalt'\n",
        "        self.salt = None\n",
        "        self.workspace = workspace\n",
        "        self.storage_account = oea_storage_account\n",
        "        self.keyvault = oea_keyvault\n",
        "        self.timezone = oea_timezone\n",
        "\n",
        "        # pull in override values if any were passed in\n",
        "        if workspace: self.workspace = workspace\n",
        "        if storage_account: self.storage_account = storage_account\n",
        "        if keyvault: self.keyvault = keyvault \n",
        "        if timezone: self.timezone = timezone\n",
        "        if logging_level: self.logging_level = logging_level    \n",
        "\n",
        "        self._initialize_logger(logging_level)\n",
        "        self.set_workspace(self.workspace)\n",
        "        spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # more info here: https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/optimize-write-for-apache-spark\n",
        "        logger.info(\"OEA initialized.\")\n",
        "\n",
        "    def _initialize_logger(self, logging_level):\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        for handler in logging.getLogger().handlers:\n",
        "            handler.setFormatter(formatter)           \n",
        "        # Customize log level for all loggers\n",
        "        logging.getLogger().setLevel(logging_level)        \n",
        "\n",
        "    def _get_secret(self, secret_name):\n",
        "        \"\"\" Retrieves the specified secret from the keyvault.\n",
        "            This method assumes that the keyvault linked service has been setup and is accessible.\n",
        "        \"\"\"\n",
        "        sc = SparkSession.builder.getOrCreate()\n",
        "        token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "        value = token_library.getSecret(self.keyvault, secret_name, self.keyvault_linked_service)        \n",
        "        return value\n",
        "\n",
        "    def _get_salt(self):\n",
        "        if not self.salt:\n",
        "            self.salt = self._get_secret(self.salt_secret_name)\n",
        "        return self.salt\n",
        "\n",
        "    def set_workspace(self, workspace_name):\n",
        "        \"\"\" Allows you to use OEA against your workspace\n",
        "            (eg, you specify Jon as workspace_name, then instead of reading in from stage1 OEA will use workspace/Jon/stage1\n",
        "        \"\"\"\n",
        "        \n",
        "        if workspace_name == 'prod' or workspace_name == 'production':\n",
        "            self.workspace = 'prod'\n",
        "            self.stage1 = 'abfss://stage1@' + self.storage_account + '.dfs.core.windows.net'\n",
        "            self.stage2 = 'abfss://stage2@' + self.storage_account + '.dfs.core.windows.net'\n",
        "            self.stage3 = 'abfss://stage3@' + self.storage_account + '.dfs.core.windows.net'\n",
        "        elif workspace_name == 'dev' or workspace_name == 'development':\n",
        "            self.workspace = 'dev'\n",
        "            self.stage1 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage1'\n",
        "            self.stage2 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage2'\n",
        "            self.stage3 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage3'\n",
        "        else:\n",
        "            self.workspace = workspace_name\n",
        "            self.stage1 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage1'\n",
        "            self.stage2 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage2'\n",
        "            self.stage3 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage3'\n",
        "        logger.info(f'Now using workspace: {self.workspace}')\n",
        "\n",
        "    def to_url(self, path):\n",
        "        \"\"\" Converts the given path into a valid url.\n",
        "            eg, convert_path('stage1/contoso_sis/student') # returns abfss://stage1@storageaccount.dfs.core.windows.net/contoso_sis/student\n",
        "            [Note that the url returned will include the sandbox location if a workspace has been set; for example, abfss://oea@storageaccount.dfs.core.windows.net/sandboxes/sam/stage1/contoso_sis/student]\n",
        "        \"\"\"\n",
        "        if not path or path == '': raise ValueError('Specified path cannot be empty.')\n",
        "        if path.startswith('abfss://'): return path # if a url is given, just return that same url (allows to_url to be invoked just in case translation may be needed)\n",
        "        path_args = path.split('/')\n",
        "        stage = path_args.pop(0)\n",
        "        if stage == 'stage1': stage = self.stage1\n",
        "        elif stage == 'stage2': stage = self.stage2\n",
        "        elif stage == 'stage3': stage = self.stage3\n",
        "        else: raise ValueError(\"Path must begin with either 'stage1', 'stage2', or 'stage3'\")\n",
        "        url = f\"{stage}/{'/'.join(path_args)}\"\n",
        "        logger.debug(f'to_url: {url}')\n",
        "        return url      \n",
        "\n",
        "    def parse_path(self, path):\n",
        "        \"\"\" Parses a path that looks like one of the following:\n",
        "                ms_insights/v0.1\n",
        "                ms_insights/v0.1/students\n",
        "\n",
        "                stage1/Transactional/ms_insights/v0.1\n",
        "                stage1/Transactional/ms_insights/v0.1/students\n",
        "            (the path must either be the path to a specific entity, or the path to the parent folder containing entities)\n",
        "            and returns a dictionary like one of the following:\n",
        "                {'stage': 'stage1', 'stage_num': '1', 'category': 'Transactional', 'source_system': 'contoso_sis', 'entity': None, 'entity_list': ['studentattendance'], 'entity_path': None, 'entity_parent_path': 'stage1/Transactional/contoso_sis/v0.1'}\n",
        "                {'stage': 'stage1', 'stage_num': '1', 'category': 'Transactional', 'source_system': 'contoso_sis', 'entity': 'studentattendance', 'entity_list': None, 'entity_path': 'stage1/Transactional/contoso_sis/v0.1/studentattendance', 'entity_parent_path': 'stage1/Transactional/contoso_sis/v0.1'}\n",
        "\n",
        "            This method assumes the standard OEA data lake, in which paths have this structure: <stage number>/<category>/<source system>/<optional version and partitioning>/<entity>/<either batch_data folder or _delta_log>\n",
        "        \"\"\"\n",
        "        if type(path) is dict: return path # this means the path was already parsed\n",
        "        \n",
        "        ar = path.split('/')\n",
        "        path_dict = {'stage':ar[0], 'stage_num':ar[0][-1], 'category':ar[1], 'source_system':ar[2], 'entity':None, 'entity_list':None, 'entity_path':None, 'entity_parent_path':None}\n",
        "\n",
        "        folders = self.get_folders(self.to_url(path))\n",
        "\n",
        "        # Identify an entity folder by the presence of the \"_delta_log\" folder in stage2 and stage3\n",
        "        if (path_dict['stage_num'] == '1' and ('additive_batch_data' in folders or 'delta_batch_data' in folders or 'snapshot_batch_data' in folders)) or ((path_dict['stage_num'] == '2' or path_dict['stage_num'] == '3') and '_delta_log' in folders):\n",
        "            path_dict['entity'] = ar[-1]\n",
        "            path_dict['entity_path'] = path\n",
        "            path_dict['entity_parent_path'] = '/'.join(ar[0:-1]) # eg, stage1/Transactional/contoso_sis/v0.1\n",
        "        else:\n",
        "            path_dict['entity_list'] = folders\n",
        "            path_dict['entity_parent_path'] = path\n",
        "\n",
        "        if path_dict['stage'] == 'stage2':\n",
        "            abbrev = path_dict['category'][0].lower() # either 'i' for Ingested or 'r' for Refined\n",
        "            path_dict['sdb_name'] = f'sdb_{self.workspace}_s{path_dict[\"stage_num\"]}{abbrev}_{path_dict[\"source_system\"].lower()}' # name of the sql db for this source (use lower case to match the naming for lake db)\n",
        "            path_dict['ldb_name'] = f'ldb_{self.workspace}_s{path_dict[\"stage_num\"]}{abbrev}_{path_dict[\"source_system\"].lower().replace(\"-\",\"_\")}' # name of the lake db for this source (spark will automatically lower case the name of the db, but we're doing it here to be explicit)\n",
        "        else:\n",
        "            path_dict['sdb_name'] = f'sdb_{self.workspace}_s{path_dict[\"stage_num\"]}_{path_dict[\"source_system\"].lower()}' # name of the sql db for this source (use lower case to match the naming for lake db)\n",
        "            path_dict['ldb_name'] = f'ldb_{self.workspace}_s{path_dict[\"stage_num\"]}_{path_dict[\"source_system\"].lower().replace(\"-\",\"_\")}' # name of the lake db for this source (spark will automatically lower case the name of the db, but we're doing it here to be explicit)\n",
        "        \n",
        "        path_dict['between_path'] = '/'.join(path_dict['entity_parent_path'].split('/')[2:]) # strip off the first 2 args in the entity parent path (eg, strip off stage1/Transactional which leaves contoso_sis/v0.1)\n",
        "\n",
        "        m = re.match(r'.*\\/(v[^\\/]+).*', path_dict['between_path'])\n",
        "        if m:\n",
        "            path_dict['version'] = m.group(1)\n",
        "            # Append the version number to the db names. First replace the '.' char with a 'p' if necessary (because a '.' is not allowed in the db name)\n",
        "            safe_version = re.sub('\\.', 'p', path_dict[\"version\"])\n",
        "            path_dict['sdb_name'] = f'{path_dict[\"sdb_name\"]}_{safe_version}'\n",
        "            path_dict['ldb_name'] = f'{path_dict[\"ldb_name\"]}_{safe_version}'\n",
        "        else:\n",
        "            path_dict['version'] = None\n",
        "\n",
        "        return path_dict\n",
        "    \n",
        "    def rm_if_exists(self, path, recursive_remove=True):\n",
        "        \"\"\" Remove a folder if it exists (defaults to use of recursive removal). \"\"\"\n",
        "        try:\n",
        "            mssparkutils.fs.rm(self.to_url(path), recursive_remove)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    def delete(self, path):\n",
        "        \"\"\" Delete a folder and everything in it. \n",
        "            This only works for non-production workspaces - to prevent accidental deletion of production data.\n",
        "        \"\"\"\n",
        "        if self.workspace == 'prod':\n",
        "            raise ValueError(\"Your current workspace is 'prod'. The 'delete' method cannot be used against the production workspace.\") \n",
        "        else:\n",
        "            self.rm_if_exists(path, True)\n",
        "\n",
        "    def ls(self, path):\n",
        "        \"\"\" List the contents of the given path. \"\"\"\n",
        "        url = self.to_url(path)\n",
        "        folders = []\n",
        "        files = []\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(url)\n",
        "            for item in items:\n",
        "                if item.isFile:\n",
        "                    files.append(item.name)\n",
        "                elif item.isDir:\n",
        "                    folders.append(item.name)\n",
        "        except Exception as e:\n",
        "            logger.warning(\"[OEA] Could not peform ls on specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
        "        return (folders, files)\n",
        "\n",
        "    def path_exists(self, path):\n",
        "        \"\"\" Returns true if path exists, false if it doesn't (no exception will be thrown). \n",
        "            eg, path_exists('stage1/mytest/v1.0')\n",
        "        \"\"\"\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(self.to_url(path))\n",
        "        except Exception as e:\n",
        "            # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def get_stage_num(self, path):\n",
        "        m = re.match(r'.*stage(\\d)/.*', path)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "        else:\n",
        "            raise ValueError(\"Path must begin with either 'stage1', 'stage2', or 'stage3'\")\n",
        "\n",
        "    def get_folders(self, path):\n",
        "        \"\"\" Return the list of folders found in the given path. \"\"\"\n",
        "        dirs = []\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(self.to_url(path))\n",
        "            for item in items:\n",
        "                #print(item.name, item.isDir, item.isFile, item.path, item.size)\n",
        "                if item.isDir:\n",
        "                    dirs.append(item.name)\n",
        "        except Exception as e:\n",
        "            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
        "        return dirs\n",
        "\n",
        "    def get_folder_size(self, path):\n",
        "        \"\"\" Return the list of folders found in the given path. \"\"\"\n",
        "        result = 0\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(self.to_url(path))\n",
        "            for item in items:\n",
        "                result += item.size\n",
        "        except Exception as e:\n",
        "            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
        "        return result\n",
        "\n",
        "    def contains_children(self, path):\n",
        "        \"\"\"Returns a boolean if the folder has children\"\"\"\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(self.to_url(path))\n",
        "            if len(items)>0:\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            #If an exception is thrown the parent folder doesn't exist\n",
        "            #return False\n",
        "            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_latest_runtime(self, path, pattern):\n",
        "        \"\"\"Gets the datetime of the latest runtime\"\"\"\n",
        "        dirs = self.get_folders(path)\n",
        "        if len(dirs) == 0:\n",
        "            return None\n",
        "\n",
        "        latest_runtime = datetime.datetime.strptime(dirs[0], pattern)\n",
        "        for i in range(len(dirs)-1):\n",
        "            curr_runtime = datetime.datetime.strptime(dirs[i+1], pattern)\n",
        "            if(curr_runtime > latest_runtime):\n",
        "                latest_runtime = curr_runtime\n",
        "        return latest_runtime\n",
        "\n",
        "    def get_latest_folder(self, path):\n",
        "        \"\"\" Gets the last folder listed in the given path. \"\"\"\n",
        "        folders = self.get_folders(path)\n",
        "        if len(folders) > 0: return folders[-1]\n",
        "        else: return None\n",
        "\n",
        "    def contains_batch_folder(self, path):\n",
        "        for name in self.get_folders(self.to_url(path)):\n",
        "            if name == 'additive_batch_data' or name == 'snapshot_batch_data' or name == 'delta_batch_data':\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def get_batch_info(self, source_path):\n",
        "        \"\"\" Given a source data path, returns a tuple with the batch type (based on the name of the folder) and file type (based on a file extension) \n",
        "            eg, get_batch_info('stage1/Transactional/sis/v1.0/students') # returns ('snapshot', 'csv')\n",
        "        \"\"\"\n",
        "        url = self.to_url(source_path)\n",
        "        folders = self.get_folders(url) #expects to find one of: additivie_batch_data, snapshot_batch_data, delta_batch_data, delete_batch_data\n",
        "        batches = []\n",
        "        for folder in folders:\n",
        "            batch_type = folder.split('_')[0]\n",
        "            if batch_type == 'additive' or batch_type == 'snapshot' or batch_type == 'delta' or batch_type == 'delete':\n",
        "                rundate_dir = self.get_latest_folder(f'{url}/{folder}')\n",
        "                data_files = self.ls(f'{url}/{folder}/{rundate_dir}')[1]\n",
        "                file_extension = data_files[0].split('.')[1]\n",
        "                batches.append((batch_type, file_extension))\n",
        "        \n",
        "        #if delete is one of the batches, reorganize the folders so it is the last batch\n",
        "        #This is done because on the initial load at least one batch has to be present before\n",
        "        #Deletion will work properly\n",
        "        if(len(batches) > 1):\n",
        "            delete_index = -1\n",
        "            for i in range(len(batches)):\n",
        "                if batches[i][0] == \"delete\":\n",
        "                    delete_index = i\n",
        "            if delete_index > -1:\n",
        "                batches = [*batches[0: delete_index],*batches[delete_index+1:len(batches)+1], batches[ delete_index]]\n",
        "        return batches       \n",
        "\n",
        "    def load(self, path):\n",
        "        df = spark.read.format('delta').load(self.to_url(path))\n",
        "        return df        \n",
        "\n",
        "    def display(self, path, limit=4):\n",
        "        df = spark.read.format('delta').load(self.to_url(path))\n",
        "        display(df.limit(limit))\n",
        "        return df\n",
        "\n",
        "    def show(self, path, limit=4):\n",
        "        df = spark.read.format('delta').load(self.to_url(path))\n",
        "        df.show(limit)\n",
        "        return df\n",
        "\n",
        "    def fix_column_names(self, df):\n",
        "        \"\"\" Fix column names to satisfy the Parquet naming requirements by substituting invalid characters with an underscore. \"\"\"\n",
        "        df_with_valid_column_names = df.select([F.col(col).alias(self.fix_column_name(col)) for col in df.columns])\n",
        "        return df_with_valid_column_names\n",
        "\n",
        "    def fix_column_name(self, column_name):\n",
        "        return re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", column_name) \n",
        "\n",
        "    def to_spark_schema(self, schema):#: list[list[str]]):\n",
        "        \"\"\" Creates a spark schema from a schema specified in the OEA schema format. \n",
        "            Example:\n",
        "            schemas['Person'] = [['Id','string','hash'],\n",
        "                                    ['CreateDate','timestamp','no-op'],\n",
        "                                    ['LastModifiedDate','timestamp','no-op']]\n",
        "            to_spark_schema(schemas['Person'])\n",
        "        \"\"\"\n",
        "        fields = []\n",
        "        for col_schema in schema:\n",
        "            col_name = col_schema[0]\n",
        "            col_dtype = col_schema[1]\n",
        "            fields.append(StructField(col_name, globals()[col_dtype.lower().capitalize() + \"Type\"](), True))\n",
        "        spark_schema = StructType(fields)\n",
        "        return spark_schema\n",
        "\n",
        "    def get_text_from_path(self, path):\n",
        "        txt = mssparkutils.fs.head(oea.to_url(path), 9000000)\n",
        "        return txt\n",
        "\n",
        "    def get_metadata_from_url(self, url):\n",
        "        csv_str = requests.get(url).text\n",
        "        metadata = self.parse_metadata_from_csv(csv_str)\n",
        "        return metadata   \n",
        "\n",
        "    def get_metadata_from_path(self, path):\n",
        "        csv_str = self.get_text_from_path(path + '/metadata.csv')\n",
        "        metadata = self.parse_metadata_from_csv(csv_str)\n",
        "        return metadata                   \n",
        "\n",
        "    def land_metadata_from_url(self, metadata_url, dataset_path):\n",
        "        metadata_str = requests.get(metadata_url).text\n",
        "        self.write(metadata_str, self._metadata_path(dataset_path))\n",
        "\n",
        "    def _metadata_path(self, dataset_path):\n",
        "        return f'stage2/Ingested/{dataset_path}/metadata.csv'\n",
        "\n",
        "    def parse_metadata_from_csv(self, csv_str):\n",
        "        \"\"\" Parses out metadata from a csv string and returns the metadata dictionary. \"\"\"\n",
        "        metadata = {}\n",
        "        current_entity = ''\n",
        "        header = None\n",
        "        for line in csv_str.splitlines():\n",
        "            line = line.strip()\n",
        "            # skip empty lines, lines that start with # (because these are comments), and lines with only commas (which is what happens if someone uses excel and leaves a row blank) \n",
        "            if len(line) == 0 or line.startswith('#') or re.match(r'^,+$', line): continue\n",
        "            ar = line.split(',')\n",
        "\n",
        "            if not header:\n",
        "                header = []\n",
        "                for column_name in ar:\n",
        "                    header.append(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", column_name))\n",
        "                continue\n",
        "            \n",
        "            # check for the start of a new entity definition\n",
        "            if ar[0] != '':\n",
        "                current_entity = ar[0]\n",
        "                metadata[current_entity] = []\n",
        "            # an attribute row must have an attribute name in the second column\n",
        "            elif len(ar[1]) > 0:\n",
        "                ar = ar[1:] # remove the first element because it will be blank\n",
        "                ar[0] = self.fix_column_name(ar[0]) # remove spaces and other illegal chars from column names\n",
        "                metadata[current_entity].append(ar)\n",
        "            else:\n",
        "                logger.info('Invalid metadata row: ' + line)\n",
        "        return metadata\n",
        "\n",
        "    def write(self, data_str, destination_path_and_filename):\n",
        "        \"\"\" Writes the given data string to a file on blob storage \"\"\"\n",
        "        destination_url = self.to_url(destination_path_and_filename)\n",
        "        mssparkutils.fs.put(destination_url, data_str, True) # Set the last parameter as True to create the file if it does not exist    \n",
        "\n",
        "    def create_run_date(self):\n",
        "        rundate = datetime.datetime.now().replace(microsecond=0) # use UTC for the datetime because when parsing it out later, spark's to_timestamp() assumes the local machine's timezone, and the timezone for the spark cluster will be UTC\n",
        "        return rundate\n",
        "\n",
        "    def land(self, data, entity_path, filename=None, batch_data_type=DELTA_BATCH_DATA, rundate=None):\n",
        "        \"\"\" Lands data in the given entity_path, adding a rundate folder.\n",
        "            eg, land(data, 'contoso/v0.1/students', 'students.csv', oea.SNAPSHOT_BATCH_DATA)\n",
        "            The data must be either a string, a pyspark DataFrame, or a pandas DataFrame.\n",
        "        \"\"\"\n",
        "        if not rundate:\n",
        "            rundate = self.create_run_date()\n",
        "            # This is a workaround for a defect in spark that occurs when using structured streaming with json files. For some reason the readStream method does not allow colons when reading from json source.\n",
        "            # (The exception it throws is --> IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: rundate=2022-11-26 21:49:05)\n",
        "            file_extension = filename.split('.')[1]\n",
        "            if file_extension == 'json':\n",
        "                rundate = str(rundate).replace(':', '-')\n",
        "\n",
        "        sink_path = f'stage1/Transactional/{entity_path}/{batch_data_type}/rundate={rundate}'\n",
        "        if isinstance(data, DataFrame):\n",
        "            data.write.mode('overwrite').format('parquet').save(self.to_url(sink_path))\n",
        "        elif isinstance(data, pd.DataFrame):\n",
        "            df = spark.createDataFrame(data)\n",
        "            df.write.mode('overwrite').format('parquet').save(self.to_url(sink_path))            \n",
        "        elif isinstance(data, str):\n",
        "            self.write(data, f'{sink_path}/{filename}')\n",
        "        else:\n",
        "            raise ValueError(f'The given data is in an unsupported format.') \n",
        "        return sink_path                  \n",
        "\n",
        "    def upsert(self, df, destination_path, primary_key='id'):\n",
        "        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "        df = df.dropDuplicates([primary_key])\n",
        "        if DeltaTable.isDeltaTable(spark, destination_url):\n",
        "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\n",
        "            #delta_table_sink.alias('sink').option('mergeSchema', 'true').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll()\n",
        "            delta_table_sink.alias('sink').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "        else:\n",
        "            logger.debug('No existing delta table found. Creating delta table.')\n",
        "            df.write.format('delta').save(destination_url)\n",
        "\n",
        "    def overwrite(self, df, destination_path, primary_key='id'):\n",
        "        \"\"\" Overwrites the existing delta table with the given dataframe.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "        df = df.dropDuplicates([primary_key])\n",
        "        df.write.format('delta').mode('overwrite').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#overwrite        \n",
        "\n",
        "    def append(self, df, destination_path, primary_key='id'):\n",
        "        \"\"\" Appends the given dataframe to the delta table in the specified destination.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "        df = df.dropDuplicates([primary_key])\n",
        "        if DeltaTable.isDeltaTable(spark, destination_url):\n",
        "            df.write.format('delta').mode('append').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#append\n",
        "        else:\n",
        "            logger.debug('No existing delta table found. Creating delta table.')\n",
        "            df.write.format('delta').save(destination_url)\n",
        "    \n",
        "    def delete_rows(self, df, destination_path, primary_key='id', batch_id=''):\n",
        "        \"\"\"Removes the entities in the given dataframe from the delta table in the specified destination\n",
        "            Will drop and refresh the lake database table\n",
        "        \"\"\"\n",
        "        ref = f'{destination_path.split(\"/\")[-1]}{batch_id}'\n",
        "        print(ref)\n",
        "        df.createOrReplaceGlobalTempView(ref)\n",
        "        source_dict = self.parse_path(destination_path)\n",
        "        db_name = source_dict['ldb_name']\n",
        "        entity_name = source_dict['entity']\n",
        "        \n",
        "        url = self.to_url(destination_path)\n",
        "        dt = spark.read.format(\"delta\").load(url)\n",
        "        dt_columns = ', '.join(f'{db_name}.{entity_name}.{col}'for col in dt.columns)\n",
        "\n",
        "        #Synapse does not yet support subqueries inside of a delete statement, so we found the quickest\n",
        "        #method for deleting entities is joining the dataframe with the table and filtering out the entities\n",
        "        #that exist in the dataframe (keeping only the entities that exist in the table)\n",
        "        df1 = spark.sql(f\"Select {dt_columns} FROM {db_name}.{entity_name} left outer join global_temp.{ref} on global_temp.{ref}.{primary_key} = {db_name}.{entity_name}.{primary_key} where isnull(global_temp.{ref}.{primary_key})\")\n",
        "        df1.write.format('delta').mode('overwrite').save(url)\n",
        "\n",
        "        #Refresh the table, otherwise the changes don't seem to be visible\n",
        "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{entity_name}\")\n",
        "        spark.sql(f\"create table if not exists {db_name}.{entity_name} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\n",
        "\n",
        "    def process(self, source_path, foreach_batch_function, options={}):\n",
        "        \"\"\" This simplifies the process of using structured streaming when processing transformations.\n",
        "            Provide a source_path and a function that receives a dataframe to work with (which will be a dataframe with data from the given source_path).\n",
        "            Use it like this...\n",
        "            def refine_contoso_dataset(df_source):\n",
        "                metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv')\n",
        "                df_pseudo, df_lookup = oea.pseudonymize(df, metadata['studentattendance'])\n",
        "                oea.upsert(df_pseudo, 'stage2/Refined/contoso_sis/v0.1/studentattendance/general')\n",
        "                oea.upsert(df_lookup, 'stage2/Refined/contoso_sis/v0.1/studentattendance/sensitive')\n",
        "            oea.process('stage2/Ingested/contoso_sis/v0.1/studentattendance', refine_contoso_dataset)             \n",
        "        \"\"\"\n",
        "        if not self.path_exists(source_path):\n",
        "            raise ValueError(f'The given path does not exist: {source_path} (which resolves to: {self.to_url(source_path)})') \n",
        "\n",
        "        def wrapped_function(df, batch_id):\n",
        "            df.persist() # cache the df so it doesn't get read in multiple times when we write to multiple destinations. See: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch\n",
        "            foreach_batch_function(df, batch_id)\n",
        "            df.unpersist()\n",
        "\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
        "        #source_path = source_path.replace(':', '\\:')\n",
        "        print(f\"source_path is: {source_path}\")\n",
        "        streaming_df = spark.readStream.load(self.to_url(source_path), **options)\n",
        "        # for more info on append vs complete vs update modes for structured streaming: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\n",
        "        query = streaming_df.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', self.to_url(source_path) + '/_checkpoints').foreachBatch(wrapped_function).start()\n",
        "        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
        "        number_of_new_inbound_rows = query.lastProgress[\"numInputRows\"]\n",
        "        logger.info(f'Number of new inbound rows processed: {number_of_new_inbound_rows}')\n",
        "        logger.debug(query.lastProgress)\n",
        "        return number_of_new_inbound_rows\n",
        "\n",
        "    def ingest_all(self, dataset_path, primary_key='id', options={}):\n",
        "        \"\"\" Ingests all the entities in the given source_path.\n",
        "            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\n",
        "            To specify options that are different from these defaults, use the options param.\n",
        "            eg, ingest('contoso_sis/v0.1') # ingests all entities found in that path\n",
        "            eg, ingest('contoso_sis/v0.1', options={'header':False}) # for CSV files that don't have a header        \n",
        "        \"\"\"\n",
        "        folders = self.get_folders(self.to_url(f'stage1/Transactional/{dataset_path}'))\n",
        "        number_of_new_inbound_rows = 0\n",
        "        for entity_name in folders:\n",
        "            number_of_new_inbound_rows += self.ingest(f'{dataset_path}/{entity_name}', primary_key, options)\n",
        "        return number_of_new_inbound_rows\n",
        "\n",
        "    def ingest(self, entity_path, primary_key='id', options={}):\n",
        "        \"\"\" Ingests the data for the entity in the given path.\n",
        "            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\n",
        "            To specify options that are different from these defaults, use the options param.\n",
        "            eg, ingest('contoso_sis/v0.1/students') # ingests all entities found in that path\n",
        "            eg, ingest('contoso_sis/v0.1/students', options={'header':False}) # for CSV files that don't have a header\n",
        "        \"\"\"\n",
        "        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\n",
        "        ingested_path = f'stage2/Ingested/{entity_path}'\n",
        "        raw_path = f'stage1/Transactional/{entity_path}'\n",
        "\n",
        "        if not self.path_exists(raw_path):\n",
        "            logger.error(f'Failed to ingest data because the given source data was not found where expected: {raw_path}')\n",
        "            return\n",
        "\n",
        "        batches = self.get_batch_info(raw_path)\n",
        "        number_of_inbound_changes = 0\n",
        "        for batch in batches:\n",
        "            batch_type = batch[0]\n",
        "            source_data_format = batch[1]\n",
        "            logger.info(f'Ingesting from: {raw_path}, batch type of: {batch_type}, source data format of: {source_data_format}')\n",
        "            source_url = self.to_url(f'{raw_path}/{batch_type}_batch_data')\n",
        "\n",
        "            if oea.get_folder_size(f'{source_url}/{self.get_latest_folder(source_url)}') > 0:\n",
        "                if batch_type == 'snapshot'or batch_type=='additive': source_url = f'{source_url}/{self.get_latest_folder(source_url)}' \n",
        "                    \n",
        "                logger.debug(f'Processing {batch_type} data from: {source_url} and writing out to: {ingested_path}')\n",
        "                if batch_type == 'snapshot':\n",
        "                    def batch_func(df, batch_id): self.overwrite(df, ingested_path, primary_key)\n",
        "                elif batch_type == 'additive':\n",
        "                    def batch_func(df, batch_id): self.append(df, ingested_path, primary_key)\n",
        "                elif batch_type == 'delta':\n",
        "                    def batch_func(df, batch_id): self.upsert(df, ingested_path, primary_key)\n",
        "                elif batch_type == \"delete\":\n",
        "                    def batch_func(df, batch_id): self.delete_rows(df, ingested_path, primary_key, batch_id)\n",
        "                else:\n",
        "                    raise ValueError(\"No valid batch folder was found at that path (expected to find a single folder with one of the following names: snapshot_batch_data, additive_batch_data, or delta_batch_data). Are you sure you have the right path?\")                      \n",
        "\n",
        "                if options == None: options = {}\n",
        "                options['format'] = source_data_format # eg, 'csv', 'json'\n",
        "                if source_data_format == 'csv' and (not 'header' in options or options['header'] == None): options['header'] = True  # default to expecting a header in csv files\n",
        "                if source_data_format == 'json' and (not 'multiline' in options or options['multiline'] == None): options['multiline'] = True # default to expecting multiline formatted json data\n",
        "\n",
        "                number_of_new_inbound_rows = self.process(source_url, batch_func, options)\n",
        "                if number_of_new_inbound_rows > 0:    \n",
        "                    self.add_to_lake_db(ingested_path)\n",
        "                number_of_inbound_changes += number_of_new_inbound_rows\n",
        "        return number_of_inbound_changes\n",
        "\n",
        "    def query(self, source_path, query_str, criteria_str=None):\n",
        "        df = self.load(source_path)\n",
        "        sqlContext.registerDataFrameAsTable(df, 'tmp_source_table')\n",
        "        if criteria_str:\n",
        "            query = f'{query_str} from tmp_source_table where {criteria_str}'\n",
        "        else:\n",
        "            query = f'{query_str} from tmp_source_table'\n",
        "        df = sqlContext.sql(query)\n",
        "        return df       \n",
        "\n",
        "    def get_latest_changes(self, source_path, sink_path):\n",
        "        \"\"\" Returns a dataframe representing the changes in the source data based on the max rundate in the sink data. \n",
        "            If the sink path is not found, all of the data from the source_path is returned (the assumption is that the sink delta table is being created for the first time).\n",
        "            eg, get_latest_changes('stage2/Ingested/contoso/v0.1/students', 'stage2/Refined/contoso/v0.1/students')\n",
        "        \"\"\"   \n",
        "        maxdatetime = None\n",
        "        try:\n",
        "            sink_df = self.query(sink_path, 'select max(rundate) maxdatetime')\n",
        "            maxdatetime = sink_df.first()['maxdatetime']\n",
        "        except AnalysisException as e:\n",
        "            # This means that there is no delta table at the sink_path yet.\n",
        "            # We'll assume that the sink delta table is being created for the first time, meaning that all of the source data should be returned.\n",
        "            pass\n",
        "\n",
        "        changes_df = self.load(source_path)\n",
        "        if maxdatetime:\n",
        "            # filter the source table for the latest changes (using the max rundate in the destination table as the watermark)\n",
        "            changes_df = changes_df.where(f\"rundate > '{maxdatetime}'\")        \n",
        "        return changes_df\n",
        "\n",
        "    def refine_all(self, dataset_path, metadata=None, primary_key='id'):\n",
        "        \"\"\" Refines all the entities in the given dataset_path.\n",
        "            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\n",
        "            To specify options that are different from these defaults, use the options param.\n",
        "            eg, ingest('contoso_sis/v0.1') # ingests all entities found in that path\n",
        "            eg, ingest('contoso_sis/v0.1', options={'header':False}) # for CSV files that don't have a header        \n",
        "        \"\"\"\n",
        "        folders = self.get_folders(self.to_url(f'stage2/Ingested/{dataset_path}'))\n",
        "        number_of_new_inbound_rows = 0\n",
        "        for entity_name in folders:\n",
        "            number_of_new_inbound_rows += self.refine(f'{dataset_path}/{entity_name}', metadata, primary_key)\n",
        "        return number_of_new_inbound_rows\n",
        "\n",
        "    def refine(self, entity_path, metadata=None, primary_key='id'):\n",
        "        source_path = f'stage2/Ingested/{entity_path}'\n",
        "        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\n",
        "        path_dict = self.parse_path(source_path)\n",
        "        sink_general_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/general/' + path_dict['entity']\n",
        "        sink_sensitive_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/sensitive/' + path_dict['entity'] + '_lookup'\n",
        "        if not metadata:\n",
        "            all_metadata = self.get_metadata_from_path(path_dict['entity_parent_path'])\n",
        "            metadata = all_metadata[path_dict['entity']]\n",
        "\n",
        "        df_changes = self.get_latest_changes(source_path, sink_general_path)\n",
        "        spark_schema = self.to_spark_schema(metadata)\n",
        "        df_changes = self.modify_schema(df_changes, spark_schema)        \n",
        "\n",
        "        if df_changes.count() > 0:\n",
        "            df_pseudo, df_lookup = self.pseudonymize(df_changes, metadata)\n",
        "            self.upsert(df_pseudo, sink_general_path, f'{primary_key}_pseudonym') # todo: remove this assumption that the primary key will always be hashed during pseduonymization\n",
        "            self.upsert(df_lookup, sink_sensitive_path, primary_key)    \n",
        "            self.add_to_lake_db(sink_general_path)\n",
        "            self.add_to_lake_db(sink_sensitive_path)\n",
        "            logger.info(f'Processed {df_changes.count()} updated rows from {source_path} into stage2/Refined')\n",
        "        else:\n",
        "            logger.info(f'No updated rows in {source_path} to process.')\n",
        "        return df_changes.count()\n",
        "\n",
        "    def modify_schema(self, df, target_spark_schema):\n",
        "        \"\"\" Transforms the given dataframe by adding appropriate schema to all the columns according to the given target schema\n",
        "            This method only handles primitive datatypes in this version.\n",
        "        \"\"\"\n",
        "        for col_name in df.columns:\n",
        "            if col_name in target_spark_schema.names:\n",
        "                target_dtype = target_spark_schema[col_name].dataType\n",
        "                df = df.withColumn(col_name, F.col(col_name).cast(target_dtype))\n",
        "        return df\n",
        "\n",
        "    def delete_dataset(self, dataset_path):\n",
        "        oea.delete(f'stage1/Transactional/{dataset_path}')\n",
        "        oea.delete(f'stage2/Ingested/{dataset_path}')\n",
        "        oea.delete(f'stage2/Refined/general/{dataset_path}')\n",
        "        oea.delete(f'stage2/Refined/sensitive/{dataset_path}_lookup') # need to add the _lookup to the end of the dataset name\n",
        "        # todo: implement the logic to do this...\n",
        "        #oea.drop_lake_db('ldb_sam_s2i_contoso_v0p1')\n",
        "        #oea.drop_lake_db('ldb_sam_s2r_contoso_v0p1')        \n",
        "\n",
        "    def load_csv(self, source_path, header=True):\n",
        "        \"\"\" Loads a csv file as a dataframe based on the path specified \"\"\"\n",
        "        options = {'format':'csv', 'header':header}\n",
        "        df = spark.read.load(self.to_url(source_path), **options)\n",
        "        return df      \n",
        "\n",
        "    def load_json(self, source_path, multiline=False):\n",
        "        \"\"\" Loads a json file as a dataframe based on the path specified \"\"\"\n",
        "        options = {'format':'json', 'multiline':multiline}\n",
        "        df = spark.read.load(self.to_url(source_path), **options)\n",
        "        return df    \n",
        "\n",
        "    def pseudonymize(self, df, metadata): #: list[list[str]]):\n",
        "        \"\"\" Performs pseudonymization of the given dataframe based on the provided metadata (in the OEA format).\n",
        "            For example, if the given df is for an entity called person, \n",
        "            2 dataframes will be returned, one called person that has hashed ids and masked fields, \n",
        "            and one called person_lookup that contains the original person_id, person_id_pseudo,\n",
        "            and the non-masked values for columns marked to be masked.           \n",
        "            The lookup table should be written to a \"sensitive\" folder in the data lake.\n",
        "            eg, df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\n",
        "            [More info on this approach here: https://learn.microsoft.com/en-us/azure/databricks/security/privacy/gdpr-delta#pseudonymize-data]\n",
        "        \"\"\"\n",
        "        salt = self._get_salt()\n",
        "        df_pseudo = df\n",
        "        df_lookup = df\n",
        "        for row in metadata:\n",
        "            col_name = row[0]\n",
        "            dtype = row[1]\n",
        "            op = row[2]\n",
        "            if op == \"hash-no-lookup\" or op == \"hnl\":\n",
        "                # This means that the lookup can be performed against a different table so no lookup is needed.\n",
        "                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
        "                df_lookup = df_lookup.drop(col_name)           \n",
        "            elif op == \"hash\" or op == 'h':\n",
        "                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
        "                df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\n",
        "            elif op == \"mask\" or op == 'm':\n",
        "                df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\n",
        "            elif op == \"partition-by\":\n",
        "                pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\n",
        "            elif op == \"no-op\" or op == 'x':\n",
        "                df_lookup = df_lookup.drop(col_name)\n",
        "        return (df_pseudo, df_lookup)\n",
        "\n",
        "    def add_to_lake_db(self, source_entity_path):\n",
        "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\n",
        "            This method will also create the lake db if it doesn't already exist.\n",
        "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\n",
        "\n",
        "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
        "        \"\"\"\n",
        "        source_dict = self.parse_path(source_entity_path)\n",
        "        db_name = source_dict['ldb_name']\n",
        "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
        "        spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\n",
        "\n",
        "    def create_lake_db(self, dataset_full_path):\n",
        "        \"\"\" Creates the lake db for the given dataset, creating an external table for each entity.\n",
        "            eg: create_lake_db('stage2/Ingested/contoso/v0.1') # creates a lake db named ldb_sam_s2i_contoso_v0p1\n",
        "        \"\"\"\n",
        "        folders = self.get_folders(self.to_url(dataset_full_path))\n",
        "        for entity_name in folders:\n",
        "            number_of_new_inbound_rows += self.add_to_lake_db(f'{dataset_full_path}/{entity_name}')\n",
        "\n",
        "\n",
        "    def drop_lake_db(self, db_name):\n",
        "        spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
        "        result = \"Database dropped: \" + db_name\n",
        "        logger.info(result)\n",
        "        return result\n",
        "\n",
        "    def create_sql_db(self, source_path):\n",
        "        \"\"\" Prints out the sql script needed for creating a sql serverless db and set of views. \"\"\"\n",
        "        source_dict = self.parse_path(source_path)\n",
        "        db_name = source_dict['sdb_name']\n",
        "        cmd = '-- Create a new sql script then execute the following in it:\\n'\n",
        "        cmd += f\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{db_name}')\\nBEGIN\\n  CREATE DATABASE {db_name};\\nEND;\\nGO\\n\"\n",
        "        cmd += f\"USE {db_name};\\nGO\\n\\n\"\n",
        "        cmd += self.create_sql_views(source_dict['entity_parent_path'])\n",
        "        print(cmd)\n",
        "\n",
        "    def create_sql_views(self, source_path):\n",
        "        cmd = ''      \n",
        "        dirs = self.get_folders(source_path)\n",
        "        for table_name in dirs:\n",
        "            cmd += f\"CREATE OR ALTER VIEW {table_name} AS\\n  SELECT * FROM OPENROWSET(BULK '{self.to_url(source_path)}/{table_name}', FORMAT='delta') AS [r];\\nGO\\n\"\n",
        "        return cmd \n",
        "\n",
        "    def drop_sql_db(self, db_name):\n",
        "        cmd = '-- Create a new sql script then execute the following in it. Alternatively, you can click on the menu next to the SQL db and select \"Delete\"\\n'\n",
        "        cmd += '-- [Note that this does not affect the data in the data lake - this will only delete the sql db that points to that data.]\\n\\n'\n",
        "        cmd += f'DROP DATABASE {db_name}'\n",
        "        print(cmd)       \n",
        "\n",
        "    def create_metadata_from_lake_db(self, db_name):\n",
        "        buffer = 'Entity Name,Attribute Name,Attribute Data Type,Pseudonymization'\n",
        "        tables = spark.catalog.listTables(db_name)\n",
        "        for table in tables:     \n",
        "            buffer += f'{table.name},,,\\n'\n",
        "            df = spark.table(f'{db_name}.{table.name}')\n",
        "            for name, dtype in df.dtypes:\n",
        "                if 'struct' in dtype: dtype = 'string'\n",
        "                buffer += f',{name},{dtype},no-op\\n'\n",
        "        return buffer\n",
        "\n",
        "    def to_simple_schema_spec(self, *dataset_full_paths):\n",
        "        \"\"\" Creates a simple-schema-spec for the delta lake at the given path(s).\n",
        "            This is used when creating a schema component for the Github repo based on an existing delta table.\n",
        "            see https://github.com/microsoft/OpenEduAnalytics/tree/main/schemas\n",
        "            \n",
        "            eg: to_simple_schema_spec('stage2/Refined/M365/v1.14/general', 'stage2/Refined/M365/v1.14/sensitive')\n",
        "        \"\"\"\n",
        "        csv_str = 'Entity Name,Attribute Name,Attribute Data Type,nullable\\n'\n",
        "        path_dict = self.parse_path(dataset_full_paths[0])\n",
        "        destination_path = f\"stage1/{path_dict['source_system']}.sss.csv\"\n",
        "\n",
        "        for dataset_full_path in dataset_full_paths:\n",
        "            folders = self.get_folders(self.to_url(dataset_full_path))\n",
        "            for entity_name in folders:\n",
        "                csv_str += f'{entity_name},,,\\n'\n",
        "                df = self.load(f'{dataset_full_path}/{entity_name}')\n",
        "                for attribute in df.schema:\n",
        "                    values = attribute.jsonValue()\n",
        "                    csv_str += f\",{values['name']},{values['type']},{values['nullable']}\\n\"\n",
        "\n",
        "        oea.write(csv_str, destination_path)\n",
        "        logger.info(f'Simple schema spec written to {destination_path}')   \n",
        "\n",
        "    def export_data(self, *dataset_full_paths):\n",
        "        \"\"\" Exports data from delta tables in the specified paths. Writes exported data to a new folder called 'extracted_data'.\n",
        "            The primary purpose for this method is to facilitate the extraction of test data to be used in an OEA schema.\n",
        "            see https://github.com/microsoft/OpenEduAnalytics/tree/main/schemas\n",
        "\n",
        "            eg: export_data('stage2/Refined/M365/v1.14/general', 'stage2/Refined/M365/v1.14/sensitive')\n",
        "        \"\"\"\n",
        "        destination_path = '/'.join(dataset_full_paths[0].split('/')[0:-1]) + '/extracted_data'\n",
        "        self.rm_if_exists(destination_path)\n",
        "        tmp_path = f'{destination_path}/tmp'\n",
        "        for dataset_full_path in dataset_full_paths:\n",
        "            logger.info(f'Gathering data from {dataset_full_path} to {destination_path}')\n",
        "            folders = self.get_folders(self.to_url(dataset_full_path))\n",
        "            for folder in folders:\n",
        "                df = self.load(f'{dataset_full_path}/{folder}')\n",
        "                df.coalesce(1).write.mode('overwrite').parquet(self.to_url(tmp_path)) # write as a single parquet file\n",
        "                filename = mssparkutils.fs.ls(self.to_url(tmp_path))[1].name\n",
        "                mssparkutils.fs.mv(f'{self.to_url(tmp_path)}/{filename}', f'{self.to_url(destination_path)}/{folder}.snappy.parquet')\n",
        "        self.rm_if_exists(tmp_path)\n",
        "\n",
        "    def _create_empty_delta_table(self, folder_path, simple_schema_spec):\n",
        "        \"\"\" Creates an empty delta table in the specified path with the schema specified by simple_schema_spec.\n",
        "        \"\"\"\n",
        "        # todo: if simple_schema_spec has a 'description' column, use 'ALTER COLUMN' to add COMMENT at the attribute level\n",
        "        folder_url = self.to_url(folder_path)\n",
        "        logger.info(f'Creating table at: {folder_url}')\n",
        "        spark_schema = self.to_spark_schema(simple_schema_spec)\n",
        "        if not DeltaTable.isDeltaTable(spark, folder_url):\n",
        "            emptyDF = spark.createDataFrame(spark.sparkContext.emptyRDD(), spark_schema)\n",
        "            emptyDF.write.format('delta').mode('overwrite').save(folder_url)\n",
        "\n",
        "    def _insert_test_data_to_delta_table(self, test_data_url, destination_path, simple_schema_spec):\n",
        "        \"\"\" Retrieves test data in parquet format from Github and writes it to the delta tables in the specified destination_path.\n",
        "        \"\"\"\n",
        "        logger.info(f'Inserting test data from: {test_data_url}\\ninto {destination_path}')\n",
        "        pdf = pd.read_parquet(test_data_url)\n",
        "        df = spark.createDataFrame(pdf)\n",
        "        spark_schema = self.to_spark_schema(simple_schema_spec)\n",
        "        df = self.modify_schema(df, spark_schema)\n",
        "        df.write.mode('overwrite').format('delta').save(self.to_url(destination_path))\n",
        "\n",
        "    def install_schema(self, sss_url, destination_path, insert_test_data = False):\n",
        "        \"\"\"\n",
        "        Installs a schema from Github.\n",
        "        Uses the given url to retrieve the simple schema spec and create empty delta lake tables as defined by the given schema.\n",
        "        If the insert_test_data flag is True, this method assumes that there is also a test_data folder in given repo url with test data in parquet format.\n",
        "        It retrieves the test data from Github and writes it to the newly created delta lake tables.\n",
        "        And then a lake db is created to make it easy to work with inside Synapse.\n",
        "\n",
        "        We assume that the schema is being created in stage2 or stage3, and that tables that end with '_lookup' need to be written to the 'sensitive' folder\n",
        "        and all other tables need to be written to 'general'.\n",
        "\n",
        "        eg: install_schema('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/schema_as_component/schemas/schema_catalog/Microsoft_Education_Insights/Microsoft_Education_Insights.sss.csv', 'stage2/Refined/contoso6/v0.1', True)\n",
        "        \"\"\"\n",
        "        logger.info(f'Installing schema as defined by: {sss_url}\\nInstalling into {destination_path}\\nFlag to insert test data is set to: {insert_test_data}')\n",
        "        test_data_url = '/'.join(sss_url.split('/')[:-1]) + '/test_data' # remove the last part of the path and add 'test_data'\n",
        "        sss = self.get_metadata_from_url(sss_url)\n",
        "        for tablename in sss.keys():\n",
        "            if tablename.endswith('_lookup'):\n",
        "                destination_table_path = f'{destination_path}/sensitive/{tablename}'\n",
        "            else:\n",
        "                destination_table_path = f'{destination_path}/general/{tablename}'\n",
        "\n",
        "            self._create_empty_delta_table(destination_table_path, sss[tablename])\n",
        "            if insert_test_data:\n",
        "                self._insert_test_data_to_delta_table(f'{test_data_url}/{tablename}.snappy.parquet', destination_table_path, sss[tablename])\n",
        "\n",
        "            # now add it to the lake db in synapse (note that the lake db will be created by self if it doesn't alread exist)\n",
        "            self.add_to_lake_db(destination_table_path)\n",
        "        logger.info('Schema installed - lake db created.')\n",
        "\n",
        "class DataLakeWriter:\n",
        "    def __init__(self, root_destination):\n",
        "        self.root_destination = root_destination\n",
        "\n",
        "    def write(self, path_and_filename, data_str, format='csv'):\n",
        "        mssparkutils.fs.append(f\"{self.root_destination}/{path_and_filename}\", data_str, True) # Set the last parameter as True to create the file if it does not exist\n",
        "\n",
        "class OpenAPIUtil:\n",
        "    \"\"\"\n",
        "    A Utility class to help processing transformations using Open API (Swagger).\n",
        "\n",
        "    Parameters:\n",
        "        1) swagger_url: URL to the OpenAPI Swaggern endpoint\n",
        "\n",
        "    Methods:\n",
        "        1) create_spark_schemas(): returns a dictionary of Spark schemas of all endpoints with entity name as the Key.\n",
        "        2) create_metadata(): returns list of dictionaries containing metadata of each field in every endpoint.\n",
        "        3) write_oea_metadata(destination_path): Writes out the OEA Metadata CSV file at the destination directory.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, swagger_url):\n",
        "        self.swagger_json = json.loads(requests.get(swagger_url).text)\n",
        "        self.metadata_headers = ['table_name', 'column_name', 'type', 'format', 'maxLength', 'required', 'items', '$ref', 'pseudonymization']\n",
        "        self.definitions = {}\n",
        "        self.metadata = {}\n",
        "        self.tables = []\n",
        "        self.schemas = {}\n",
        "        self.dependency_dict = {}\n",
        "        self.dependency_order = []\n",
        "        self.visited = {}\n",
        "\n",
        "    def get_reference(self, row):\n",
        "        if(row['type'] == 'array'):\n",
        "            reference = row['items']['$ref']\n",
        "        elif(row['$ref'] != None):\n",
        "           reference = row['$ref']\n",
        "        else:\n",
        "            return None\n",
        "        return reference.split('/')[-1].split('_')[-1]\n",
        "\n",
        "    def pluralize(self, noun):\n",
        "        if noun == 'person' : return 'people'\n",
        "        if noun == 'survey' : return 'surveys'\n",
        "        if re.search('[sxz]$', noun):\n",
        "            return re.sub('$', 'es', noun)\n",
        "        if re.search('y$', noun):\n",
        "            return re.sub('y$', 'ies', noun)\n",
        "        return noun + 's'\n",
        "\n",
        "    def get_data_type(self, dtype, format):\n",
        "        if(dtype == 'string'):\n",
        "            if(format == 'date'):\n",
        "                return DateType()\n",
        "            if(format == 'date-time'):\n",
        "                return TimestampType()\n",
        "            return StringType()\n",
        "        if(dtype == 'integer'):\n",
        "            return IntegerType()\n",
        "        if(dtype == 'number'):\n",
        "            return DecimalType()\n",
        "        if(dtype == 'boolean'):\n",
        "            return BooleanType()\n",
        "\n",
        "    def create_definitions(self):\n",
        "        for entity in self.swagger_json['definitions']:\n",
        "            properties = self.swagger_json['definitions'][entity]['properties']\n",
        "            table_name = entity.split('_')[-1]\n",
        "            table_schema = {}\n",
        "\n",
        "            for prop in properties:\n",
        "                if 'description' in properties[prop].keys():\n",
        "                    properties[prop].pop('description')\n",
        "                field_info = properties[prop]\n",
        "                if 'required' in self.swagger_json['definitions'][entity].keys():\n",
        "                    field_info['required'] = True if prop in self.swagger_json['definitions'][entity]['required'] else False\n",
        "                else:\n",
        "                    field_info['required'] = False\n",
        "                field_info['table_name'] = entity.split('_')[-1]\n",
        "                field_info['column_name'] = prop\n",
        "                if 'x-Ed-Fi-pseudonymization' in field_info:\n",
        "                    field_info['pseudonymization'] = field_info['x-Ed-Fi-pseudonymization']\n",
        "                    field_info.pop('x-Ed-Fi-pseudonymization')\n",
        "                for header in [x for x in self.metadata_headers if x not in field_info] : field_info[header] = None\n",
        "                table_schema[prop] = field_info\n",
        "\n",
        "            self.definitions[table_name] = table_schema\n",
        "        self.tables = [x for x in self.definitions.keys()]\n",
        "\n",
        "    def create_metadata(self):\n",
        "        if(len(self.schemas) == 0):\n",
        "            self.create_spark_schemas()\n",
        "        for table_name in self.dependency_order:\n",
        "            table_metadata = []\n",
        "            for col_name in self.definitions[table_name]:\n",
        "                col_schema = self.definitions[table_name][col_name]\n",
        "                key = self.pluralize(table_name)\n",
        "\n",
        "                if 'x-Ed-Fi-fields-to-pluck' in col_schema and col_schema['x-Ed-Fi-fields-to-pluck'] != [\"*\"]:\n",
        "                    referenced_table = self.get_reference(col_schema)\n",
        "                    table_metadata += [x for x in self.metadata[self.pluralize(referenced_table)] if x[0] in col_schema['x-Ed-Fi-fields-to-pluck']]\n",
        "\n",
        "                elif 'x-Ed-Fi-explode' in col_schema and col_schema['x-Ed-Fi-explode']:\n",
        "                    referenced_table = self.get_reference(col_schema)\n",
        "                    table_metadata += self.metadata[self.pluralize(referenced_table)]\n",
        "\n",
        "                else:\n",
        "                    op = self.definitions[table_name][col_name]['pseudonymization']\n",
        "                    if op == None: op = 'no-op'\n",
        "                    table_metadata.append([col_name, self.schemas[key][col_name].dataType.typeName(), op])\n",
        "            self.metadata[self.pluralize(table_name)] = table_metadata\n",
        "        return self.metadata\n",
        "\n",
        "    def write_oea_metadata(self, destination_path):\n",
        "        if(self.metadata == []):\n",
        "            self.create_metadata()\n",
        "        oea_metadata = []\n",
        "        for table_name in self.metadata:\n",
        "            oea_metadata.append([table_name, None, None, None])\n",
        "            for col_metadata in self.metadata[table_name]:\n",
        "                oea_metadata.append([None] + col_metadata)\n",
        "        metadata_df = spark.createDataFrame(oea_metadata, ['Entity Name','Attribute Name','Attribute Data Type','Pseudonymization'])\n",
        "        metadata_df.coalesce(1).write.format('csv').save(destination_path)\n",
        "\n",
        "    def create_dependency_dict(self):\n",
        "        for table_name in self.definitions:\n",
        "            for column_name in self.definitions[table_name]:\n",
        "                column_info = self.definitions[table_name][column_name]\n",
        "                referenced_table = self.get_reference(column_info)\n",
        "                if(referenced_table is None):\n",
        "                    continue\n",
        "                if table_name not in self.dependency_dict:\n",
        "                    self.dependency_dict[table_name] = [referenced_table]\n",
        "                elif referenced_table not in self.dependency_dict[table_name]:\n",
        "                    self.dependency_dict[table_name].append(referenced_table)\n",
        "\n",
        "    def dfs(self, table_name):\n",
        "        self.visited[table_name] = True\n",
        "        if table_name not in self.dependency_dict:\n",
        "            self.dependency_order.append(table_name)\n",
        "            return\n",
        "\n",
        "        for dependent_table in self.dependency_dict[table_name]:\n",
        "            if(self.visited[dependent_table] is False):\n",
        "                self.dfs(dependent_table)\n",
        "            if(self.visited[dependent_table] is False):\n",
        "                self.dependency_order.append(dependent_table)\n",
        "\n",
        "        self.dependency_order.append(table_name)\n",
        "\n",
        "    def create_dependency_order(self):\n",
        "        for table_name in self.tables:\n",
        "            self.visited[table_name] = False\n",
        "        for table_name in self.tables:\n",
        "            if(self.visited[table_name] is False):\n",
        "                self.dfs(table_name)\n",
        "\n",
        "    def create_spark_schemas_from_definitions(self):\n",
        "        for entity in self.dependency_order:\n",
        "            table_schema = self.definitions[entity]\n",
        "            spark_schema = []\n",
        "            if(entity == 'localEducationAgencyReference'):\n",
        "                print(entity)\n",
        "            for col_name in table_schema:\n",
        "                col_metadata = {}\n",
        "                if('pseudonymization' in table_schema[col_name]): col_metadata['pseudonymization'] = table_schema[col_name]['pseudonymization']\n",
        "                if('x-Ed-Fi-isIdentity' in table_schema[col_name]): col_metadata['x-Ed-Fi-isIdentity'] = table_schema[col_name]['x-Ed-Fi-isIdentity']\n",
        "                \n",
        "                col_metadata['required'] = table_schema[col_name]['required']\n",
        "                referenced_table = self.get_reference(table_schema[col_name])\n",
        "                if table_schema[col_name]['type'] == 'array':\n",
        "                    datatype = ArrayType(self.schemas[self.pluralize(referenced_table)])\n",
        "                    if('x-Ed-Fi-explode' in table_schema[col_name]):\n",
        "                        col_metadata['x-Ed-Fi-explode'] = table_schema[col_name]['x-Ed-Fi-explode']\n",
        "                elif table_schema[col_name]['$ref'] != None:\n",
        "                    datatype = self.schemas[self.pluralize(referenced_table)]\n",
        "                    if('x-Ed-Fi-fields-to-pluck' in table_schema[col_name]):\n",
        "                        col_metadata['x-Ed-Fi-fields-to-pluck'] = table_schema[col_name]['x-Ed-Fi-fields-to-pluck']\n",
        "                else:\n",
        "                    datatype = self.get_data_type(table_schema[col_name]['type'], table_schema[col_name]['format'])\n",
        "                col_spark_schema = StructField(col_name, datatype, not(table_schema[col_name]['required']))\n",
        "                col_spark_schema.metadata = col_metadata\n",
        "                spark_schema.append(col_spark_schema)\n",
        "            self.schemas[self.pluralize(entity)] = StructType(spark_schema)\n",
        "\n",
        "    def create_spark_schemas(self):\n",
        "        if(len(self.schemas) == 0):\n",
        "            self.create_definitions()\n",
        "            self.create_dependency_dict()\n",
        "            self.create_dependency_order()\n",
        "            self.create_spark_schemas_from_definitions()\n",
        "        return self.schemas\n",
        "\n",
        "oea = OEA()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
