{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "from delta.tables import DeltaTable\n",
        "from notebookutils import mssparkutils\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType, ShortType, DateType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "import logging\n",
        "import pandas as pd\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import datetime\n",
        "import pytz\n",
        "import random\n",
        "import io\n",
        "import urllib.request\n",
        "\n",
        "logger = logging.getLogger('OEA')\n",
        "\n",
        "class OEA:\n",
        "    \"\"\" OEA (Open Education Analytics) framework simplifies the process of working with large data sets within the context of a lakehouse architecture.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, storage_account='', salt='', timezone='US/Eastern', logging_level=logging.INFO):\n",
        "        spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # more info here: https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/optimize-write-for-apache-spark\n",
        "        if storage_account:\n",
        "            self.storage_account = storage_account\n",
        "        else:\n",
        "            oea_id = mssparkutils.env.getWorkspaceName()[8:] # extracts the OEA id for this OEA instance from the synapse workspace name (based on OEA naming convention)\n",
        "            self.storage_account = 'stoea' + oea_id # sets the name of the storage account based on OEA naming convention\n",
        "            self.keyvault = 'kv-oea-' + oea_id\n",
        "        self.keyvault_linked_service = 'LS_KeyVault_OEA'\n",
        "        self.serverless_sql_endpoint = mssparkutils.env.getWorkspaceName() + '-ondemand.sql.azuresynapse.net'\n",
        "        self._initialize_stage_paths()\n",
        "        self._initialize_logger(logging_level)\n",
        "        self.salt = salt\n",
        "        self.timezone = timezone\n",
        "\n",
        "        # todo: decide if this is needed (maybe it's something we should introduce again later)\n",
        "        #self.framework_path = 'abfss://synapse-workspace@' + self.storage_account + '.dfs.core.windows.net/oea_framework'\n",
        "        # Initialize framework db\n",
        "        #spark.sql(f\"CREATE DATABASE IF NOT EXISTS oea\")\n",
        "        #spark.sql(f\"CREATE TABLE IF NOT EXISTS oea.env (name string not null, value string not null, description string) USING DELTA LOCATION '{self.framework_path}/db/env'\")\n",
        "        #df = spark.sql(\"select value from oea.env where name='storage_account'\")\n",
        "        #if df.first(): spark.sql(f\"UPDATE oea.env set value='{self.storage_account}' where name='storage_account'\")\n",
        "        #else: spark.sql(f\"INSERT INTO oea.env VALUES ('storage_account', '{self.storage_account}', 'The name of the data lake storage account for this OEA instance.')\")\n",
        "        #spark.sql(f\"CREATE TABLE IF NOT EXISTS OEA.watermark (source string not null, entity string not null, watermark timestamp not null) USING DELTA LOCATION '{self.framework_path}/db/watermark'\")\n",
        "\n",
        "        logger.info(\"OEA initialized.\")\n",
        "\n",
        "    def _initialize_stage_paths(self):\n",
        "        self.stage1 = 'abfss://stage1@' + self.storage_account + '.dfs.core.windows.net'\n",
        "        self.stage2 = 'abfss://stage2@' + self.storage_account + '.dfs.core.windows.net'\n",
        "        self.stage3 = 'abfss://stage3@' + self.storage_account + '.dfs.core.windows.net'\n",
        "\n",
        "    def _initialize_logger(self, logging_level):\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        for handler in logging.getLogger().handlers:\n",
        "            handler.setFormatter(formatter)           \n",
        "        # Customize log level for all loggers\n",
        "        logging.getLogger().setLevel(logging_level)        \n",
        "\n",
        "    def use_workspace(self, workspace_name):\n",
        "        \"\"\" Allows you to use OEA against your workspace\n",
        "            (eg, you specify Jon as workspace_name, then instead of reading in from stage1 OEA will use workspace/Jon/stage1\n",
        "        \"\"\"\n",
        "        self.stage1 = f'abfss://workspace@{self.storage_account}.dfs.core.windows.net/{workspace_name}/stage1'\n",
        "        self.stage2 = f'abfss://workspace@{self.storage_account}.dfs.core.windows.net/{workspace_name}/stage2'\n",
        "        self.stage3 = f'abfss://workspace@{self.storage_account}.dfs.core.windows.net/{workspace_name}/stage3'\n",
        "        logger.info(f'Now using workspace: {workspace_name}')\n",
        "\n",
        "    def stop_using_workspace(self): \n",
        "        \"\"\" Resets OEA to use the standard stage1, stage2, stage3 paths instead of the workspace lake (see use_workspace) \"\"\"\n",
        "        self._initialize_stage_paths()\n",
        "\n",
        "    def to_url(self, path):\n",
        "        \"\"\" Converts the given path into a valid url.\n",
        "            eg, convert_path('stage1/contoso_sis/student') # returns abfss://stage1@storageaccount.dfs.core.windows.net/contoso_sis/student\n",
        "            [Note that if \"use_sandbox\" has been invoked, the url returned will be something like abfss://dev@storageaccount.dfs.core.windows.net/sandbox1/stage1/contoso_sis/student]\n",
        "        \"\"\"\n",
        "        if path.startswith('abfss://'): return path # if a url is given, just return that same url (allows to_url to be invoked just in case translation may be needed)\n",
        "        path_args = path.split('/')\n",
        "        stage = path_args.pop(0)\n",
        "        if stage == 'stage1': stage = self.stage1\n",
        "        elif stage == 'stage2': stage = self.stage2\n",
        "        elif stage == 'stage3': stage = self.stage3\n",
        "        else: raise ValueError(\"Path must begin with either 'stage1', 'stage2', or 'stage3'\")\n",
        "        url = f\"{stage}/{'/'.join(path_args)}\"\n",
        "        logger.debug(f'to_url: {url}')\n",
        "        return url\n",
        "\n",
        "    def parse_path(self, path):\n",
        "        \"\"\" Parses a path that looks like one of the following:\n",
        "                stage1/Transactional/ms_insights/v0.1\n",
        "                stage1/Transactional/ms_insights/v0.1/students\n",
        "            (the path must either be the path to a specific entity, or the path to the parent folder containing entities)\n",
        "            and returns a dictionary like one of the following:\n",
        "                {'stage': 'stage1', 'stage_num': '1', 'category': 'Transactional', 'source_system': 'contoso_sis', 'entity': None, 'entity_list': ['studentattendance'], 'entity_path': None, 'entity_parent_path': 'stage1/Transactional/contoso_sis/v0.1'}\n",
        "                {'stage': 'stage1', 'stage_num': '1', 'category': 'Transactional', 'source_system': 'contoso_sis', 'entity': 'studentattendance', 'entity_list': None, 'entity_path': 'stage1/Transactional/contoso_sis/v0.1/studentattendance', 'entity_parent_path': 'stage1/Transactional/contoso_sis/v0.1'}\n",
        "\n",
        "            This method assumes the standard OEA data lake, in which paths have this structure: <stage number>/<category>/<source system>/<optional version and partitioning>/<entity>/<either batch_data folder or _delta_log>\n",
        "        \"\"\"\n",
        "        if type(path) is dict: return path # this means the path was already parsed\n",
        "        \n",
        "        ar = path.split('/')\n",
        "        path_dict = {'stage':ar[0], 'stage_num':ar[0][-1], 'category':ar[1], 'source_system':ar[2], 'entity':None, 'entity_list':None, 'entity_path':None, 'entity_parent_path':None}\n",
        "\n",
        "        folders = self.get_folders(self.to_url(path))\n",
        "        print(folders)\n",
        "        # Identify an entity folder by the presence of the \"_delta_log\" folder in stage2 and stage3\n",
        "        if (path_dict['stage_num'] == '1' and ('additive_batch_data' in folders[0] or 'delta_batch_data' in folders[0] or 'snapshot_batch_data' in folders[0])) or ((path_dict['stage_num'] == '2' or path_dict['stage_num'] == '3') and '_delta_log' in folders[0]):\n",
        "            path_dict['entity'] = ar[-1]\n",
        "            path_dict['entity_path'] = path\n",
        "            path_dict['entity_parent_path'] = '/'.join(ar[0:-1]) # eg, stage1/Transactional/contoso_sis/v0.1\n",
        "        else:\n",
        "            path_dict['entity_list'] = folders\n",
        "            path_dict['entity_parent_path'] = path\n",
        "\n",
        "        path_dict['sdb_name'] = f'sdb_s{path_dict[\"stage_num\"]}_{path_dict[\"source_system\"].lower()}' # name of the sql db for this source (use lower case to match the naming for lake db)\n",
        "        path_dict['ldb_name'] = f'ldb_s{path_dict[\"stage_num\"]}_{path_dict[\"source_system\"].lower()}' # name of the lake db for this source (spark will automatically lower case the name of the db, but we're doing it here to be explicit)\n",
        "        path_dict['between_path'] = '/'.join(path_dict['entity_parent_path'].split('/')[2:]) # strip off the first 2 args in the entity parent path (eg, strip off stage1/Transactional which leaves contoso_sis/v0.1)\n",
        "        \n",
        "        m = re.match(r'.*\\/(v[^\\/]+).*', path_dict['between_path'])\n",
        "        if m:\n",
        "            path_dict['version'] = m.group(1)\n",
        "            # Append the version number to the db names\n",
        "            path_dict['sdb_name'] = f'{path_dict[\"sdb_name\"]}_{path_dict[\"version\"]}'\n",
        "            path_dict['ldb_name'] = f'{path_dict[\"ldb_name\"]}_{path_dict[\"version\"]}'\n",
        "        else:\n",
        "            path_dict['version'] = None\n",
        "\n",
        "        #print(path_dict)\n",
        "        return path_dict\n",
        "\n",
        "    def rm_if_exists(self, path, recursive_remove=True):\n",
        "        \"\"\" Remove a folder if it exists (defaults to use of recursive removal). \"\"\"\n",
        "        try:\n",
        "            mssparkutils.fs.rm(self.to_url(path), recursive_remove)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    def ls(self, path):\n",
        "        \"\"\" List the contents of the given path. \"\"\"\n",
        "        url = self.to_url(path)\n",
        "        folders = []\n",
        "        files = []\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(url)\n",
        "            for item in items:\n",
        "                if item.isFile:\n",
        "                    files.append(item.name)\n",
        "                elif item.isDir:\n",
        "                    folders.append(item.name)\n",
        "        except Exception as e:\n",
        "            logger.warning(\"[OEA] Could not peform ls on specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
        "        return (folders, files)\n",
        "\n",
        "    def path_exists(self, path):\n",
        "        \"\"\" Returns true if path exists, false if it doesn't (no exception will be thrown). \n",
        "            eg, path_exists('stage1/mytest/v1.0')\n",
        "        \"\"\"\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(self.to_url(path))\n",
        "        except Exception as e:\n",
        "            # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def get_stage_num(self, path):\n",
        "        m = re.match(r'.*stage(\\d)/.*', path)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "        else:\n",
        "            raise ValueError(\"Path must begin with either 'stage1', 'stage2', or 'stage3'\")\n",
        "\n",
        "    def get_folders(self, path):\n",
        "        \"\"\" Return the list of folders found in the given path. \"\"\"\n",
        "        dirs = []\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(self.to_url(path))\n",
        "            for item in items:\n",
        "                #print(item.name, item.isDir, item.isFile, item.path, item.size)\n",
        "                if item.isDir:\n",
        "                    dirs.append(item.name)\n",
        "        except Exception as e:\n",
        "            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
        "        return dirs\n",
        "\n",
        "    def get_latest_folder(self, path):\n",
        "        \"\"\" Gets the last folder listed in the given path. \"\"\"\n",
        "        folders = self.get_folders(path)\n",
        "        if len(folders) > 0: return folders[-1]\n",
        "        else: return None\n",
        "\n",
        "    def contains_batch_folder(self, path):\n",
        "        for name in self.get_folders(self.to_url(path)):\n",
        "            if name == 'additive_batch_data' or name == 'snapshot_batch_data' or name == 'delta_batch_data':\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def get_batch_info(self, source_path):\n",
        "        \"\"\" Given a source data path, returns a tuple with the batch type (based on the name of the folder) and file type (based on a file extension) \n",
        "            eg, get_batch_info('stage1/Transactional/sis/v1.0/students') # returns ('snapshot', 'csv')\n",
        "        \"\"\"\n",
        "        url = self.to_url(source_path)\n",
        "        source_folder_name = self.get_latest_folder(url) #expects to find one of: additivie_batch_data, snapshot_batch_data, delta_batch_data\n",
        "        batch_type = source_folder_name.split('_')[0]\n",
        "\n",
        "        rundate_dir = self.get_latest_folder(f'{url}/{source_folder_name}')\n",
        "        data_files = self.ls(f'{url}/{source_folder_name}/{rundate_dir}')[1]\n",
        "        file_extension = data_files[0].split('.')[1]\n",
        "        return batch_type, file_extension        \n",
        "\n",
        "    def load(self, path):\n",
        "        df = spark.read.format('delta').load(self.to_url(path))\n",
        "        return df        \n",
        "\n",
        "    def display(self, path, limit=4):\n",
        "        df = spark.read.format('delta').load(self.to_url(path))\n",
        "        display(df.limit(limit))\n",
        "        return df\n",
        "\n",
        "    def show(self, path, limit=4):\n",
        "        df = spark.read.format('delta').load(self.to_url(path))\n",
        "        df.show(limit)\n",
        "        return df\n",
        "\n",
        "    def fix_column_names(self, df):\n",
        "        \"\"\" Fix column names to satisfy the Parquet naming requirements by substituting invalid characters with an underscore. \"\"\"\n",
        "        df_with_valid_column_names = df.select([F.col(col).alias(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", col)) for col in df.columns])\n",
        "        return df_with_valid_column_names\n",
        "\n",
        "    def to_spark_schema(self, schema):#: list[list[str]]):\n",
        "        \"\"\" Creates a spark schema from a schema specified in the OEA schema format. \n",
        "            Example:\n",
        "            schemas['Person'] = [['Id','string','hash'],\n",
        "                                    ['CreateDate','timestamp','no-op'],\n",
        "                                    ['LastModifiedDate','timestamp','no-op']]\n",
        "            to_spark_schema(schemas['Person'])\n",
        "        \"\"\"\n",
        "        fields = []\n",
        "        for col_name, dtype, op in schema:\n",
        "            fields.append(StructField(col_name, globals()[dtype.lower().capitalize() + \"Type\"](), True))\n",
        "        spark_schema = StructType(fields)\n",
        "        return spark_schema\n",
        "\n",
        "    def get_text_from_url(self, url):\n",
        "        \"\"\" Retrieves the text doc at the given url. \n",
        "            eg: get_text_from_url(\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv\")\n",
        "        \"\"\"\n",
        "        response = urllib.request.urlopen(url)\n",
        "        str = response.read().decode('utf-8')  \n",
        "        return str\n",
        "\n",
        "    def parse_metadata_from_csv(self, csv_str):\n",
        "        \"\"\" Parses out metadata from a csv string and returns the metadata dictionary. \"\"\"\n",
        "        metadata = {}\n",
        "        current_entity = ''\n",
        "        header = None\n",
        "        for line in csv_str.splitlines():\n",
        "            line = line.strip()\n",
        "            # skip empty lines, lines that start with # (because these are comments), and lines with only commas (which is what happens if someone uses excel and leaves a row blank) \n",
        "            if len(line) == 0 or line.startswith('#') or re.match(r'^,+$', line): continue\n",
        "            ar = line.split(',')\n",
        "\n",
        "            if not header:\n",
        "                header = []\n",
        "                for column_name in ar:\n",
        "                    header.append(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", column_name))\n",
        "                continue\n",
        "            \n",
        "            # check for the start of a new entity definition\n",
        "            if ar[0] != '':\n",
        "                current_entity = ar[0]\n",
        "                metadata[current_entity] = []\n",
        "            # an attribute row must have an attribute name in the second column\n",
        "            elif len(ar[1]) > 0:\n",
        "                ar = ar[1:] # remove the first element because it will be blank\n",
        "                metadata[current_entity].append(ar)\n",
        "            else:\n",
        "                logger.info('Invalid metadata row: ' + line)\n",
        "        return metadata\n",
        "\n",
        "    def write(self, data_str, destination_path_and_filename):\n",
        "        \"\"\" Writes the given data string to a file on blob storage \"\"\"\n",
        "        destination_url = self.to_url(destination_path_and_filename)\n",
        "        mssparkutils.fs.put(destination_url, data_str, True) # Set the last parameter as True to create the file if it does not exist\n",
        "\n",
        "    def get_metadata_from_url(self, url):\n",
        "        csv_str = self.get_text_from_url(url)\n",
        "        metadata = self.parse_metadata_from_csv(csv_str)\n",
        "        return metadata        \n",
        "\n",
        "    def create_run_date(self, date_str=None):  \n",
        "        \"\"\" Creates a datetime string in a format like 2022-09-30T14-51-02\n",
        "            You can pass in a date_str like 2022-09-30 and it will return a datetime string.\n",
        "            If no date_str is passed in, the the datetime string returned will be the current datetime for the default timezone.\n",
        "        \"\"\"  \n",
        "        if not date_str: \n",
        "            rundate = datetime.datetime.now(pytz.timezone(self.timezone))    \n",
        "        elif re.match(r'^\\d\\d\\d\\d-\\d\\d-\\d\\dT\\d\\d-\\d\\d-\\d\\d$', date_str.strip()):\n",
        "            return date_str # return the string passed in since it was already formatted correctly (allows this method to be called liberally)\n",
        "        else:\n",
        "            rundate = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
        "        return rundate.strftime('%Y-%m-%dT%H-%M-%S') # Path names can't have a colon - https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/site/markdown/filesystem/introduction.md#path-names\n",
        "\n",
        "    def land_data(self, data, destination_path, destination_filename, rundate=None):\n",
        "        \"\"\" Lands data in the given destination_path, adding a rundate folder.\n",
        "        \"\"\"\n",
        "        rundate = self.create_run_date(rundate)\n",
        "        destination_path = f'{destination_path}/rundate={rundate}/{destination_filename}'\n",
        "        self.write(data, destination_path)\n",
        "        return destination_path\n",
        "\n",
        "    def land_data_from_url(self, url, destination_path, rundate=None):\n",
        "        \"\"\" Pulls data from the given url and lands it in the specified destination path.\n",
        "            eg, land_data_from_url('https://contoso.com/testdata/students.csv', 'stage1/Transactional/contoso_sis/v0.1/students')\n",
        "        \"\"\"\n",
        "        filename = url.split('/')[-1] # assumes the last element in the url is the name of the file (eg, myfile.csv)\n",
        "        data = self.get_text_from_url(url)\n",
        "        destination_path = self.land_data(data, destination_path, filename, rundate)\n",
        "        return destination_path\n",
        "\n",
        "    def upsert(self, df, destination_path, primary_key='id'):\n",
        "        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "        if DeltaTable.isDeltaTable(spark, destination_url):\n",
        "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\n",
        "            #delta_table_sink.alias('sink').option('mergeSchema', 'true').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll()\n",
        "            delta_table_sink.alias('sink').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "        else:\n",
        "            logger.debug('No existing delta table found. Creating delta table.')\n",
        "            df.write.format('delta').save(destination_url)\n",
        "\n",
        "    def overwrite(self, df, destination_path):\n",
        "        \"\"\" Overwrites the existing delta table with the given dataframe.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "        df.write.format('delta').mode('overwrite').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#overwrite        \n",
        "\n",
        "    def append(self, df, destination_path):\n",
        "        \"\"\" Appends the given dataframe to the delta table in the specified destination.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "        if DeltaTable.isDeltaTable(spark, destination_url):\n",
        "            df.write.format('delta').mode('append').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#append\n",
        "        else:\n",
        "            logger.debug('No existing delta table found. Creating delta table.')\n",
        "            df.write.format('delta').save(destination_url)\n",
        "\n",
        "    def process(self, source_path, foreach_batch_function, options={}):\n",
        "        \"\"\" This simplifies the process of using structured streaming when processing transformations.\n",
        "            Provide a source_path and a function that receives a dataframe to work with (which will be a dataframe with data from the given source_path).\n",
        "            Use it like this...\n",
        "            def refine_contoso_dataset(df_source):\n",
        "                metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv')\n",
        "                df_pseudo, df_lookup = oea.pseudonymize(df, metadata['studentattendance'])\n",
        "                oea.upsert(df_pseudo, 'stage2/Refined/contoso_sis/v0.1/studentattendance/general')\n",
        "                oea.upsert(df_lookup, 'stage2/Refined/contoso_sis/v0.1/studentattendance/sensitive')\n",
        "            oea.process('stage2/Ingested/contoso_sis/v0.1/studentattendance', refine_contoso_dataset)             \n",
        "        \"\"\"\n",
        "        def wrapped_function(df, batch_id):\n",
        "            df.persist() # cache the df so it doesn't get read in multiple times when we write to multiple destinations. See: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch\n",
        "            foreach_batch_function(df)\n",
        "            df.unpersist()\n",
        "\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
        "        streaming_df = spark.readStream.load(self.to_url(source_path), **options)\n",
        "        # for more info on append vs complete vs update modes for structured streaming: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\n",
        "        query = streaming_df.writeStream.outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.to_url(source_path) + '/_checkpoints').foreachBatch(wrapped_function).start()\n",
        "        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
        "        number_of_new_inbound_rows = query.lastProgress[\"numInputRows\"]\n",
        "        logger.info(f'Number of new inbound rows processed: {number_of_new_inbound_rows}')\n",
        "        logger.debug(query.lastProgress)\n",
        "        return number_of_new_inbound_rows\n",
        "\n",
        "    def ingest(self, source_path, primary_key='id', options={}):\n",
        "        \"\"\" Ingests all the entities in the given source_path, or ingests the data for the entity if given an entity path.\n",
        "            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\n",
        "            To specify options that are different from these defaults, use the options param.\n",
        "            eg, ingest('stage1/Transactional/contoso_sis/v0.1') # ingests all entities found in that path\n",
        "            eg, ingest('stage1/Transactional/contoso_sis/v0.1/studentattendance') # ingests the batch data for the single entity in this path\n",
        "            eg, ingest('stage1/Transactional/contoso_sis/v0.1', options={'header':False}) # for CSV files that don't have a header\n",
        "            eg, ingest('stage1/Transactional/contoso_sis/v0.1/studentattendance', options={'multiline':True}) # for JSON files that are not a JSON doc per row\n",
        "        \"\"\"\n",
        "        source_dict = self.parse_path(source_path)\n",
        "        if source_dict['entity']:\n",
        "            number_of_new_inbound_rows = self._ingest_entity(source_dict, primary_key, options)\n",
        "        else:\n",
        "            number_of_new_inbound_rows = 0\n",
        "            for name in source_dict['entity_list']:\n",
        "                number_of_new_inbound_rows += self._ingest_entity(source_dict, primary_key, options)\n",
        "        return number_of_new_inbound_rows\n",
        "\n",
        "    def _ingest_entity(self, source_dict, primary_key, options):\n",
        "        \"\"\" Performs the basic data ingestion - ingesting incoming batch data from stage1 into delta lake format in stage2. \"\"\"\n",
        "        destination_path = f'stage2/Ingested/{source_dict[\"between_path\"]}/{source_dict[\"entity\"]}'\n",
        "        batch_type, source_data_format = self.get_batch_info(source_dict['entity_path'])\n",
        "        source_url = self.to_url(f'{source_dict[\"entity_path\"]}/{batch_type}_batch_data')\n",
        "\n",
        "        if batch_type == 'snapshot': source_url = f'{source_url}/{self.get_latest_folder(source_url)}' \n",
        "            \n",
        "        logger.info(f'Processing {batch_type} data from: {source_url} and writing out to: {destination_path}')\n",
        "        if batch_type == 'snapshot':\n",
        "            def batch_func(df): self.overwrite(df, destination_path)\n",
        "        elif batch_type == 'additive':\n",
        "            def batch_func(df): self.append(df, destination_path)\n",
        "        elif batch_type == 'delta':\n",
        "            def batch_func(df): self.upsert(df, destination_path, primary_key)\n",
        "        else:\n",
        "            raise ValueError(\"No valid batch folder was found at that path (expected to find a single folder with one of the following names: snapshot_batch_data, additive_batch_data, or delta_batch_data). Are you sure you have the right path?\")                      \n",
        "\n",
        "        if options == None: options = {}\n",
        "        options['format'] = source_data_format # eg, 'csv', 'json'\n",
        "        if source_data_format == 'csv' and options['header'] == None: options['header'] = True  # default to expecting a header in csv files\n",
        "\n",
        "        number_of_new_inbound_rows = self.process(source_url, batch_func, options)       \n",
        "        self.add_to_lake_db(destination_path)\n",
        "        return number_of_new_inbound_rows\n",
        "\n",
        "    def load_csv(self, source_path, header=True):\n",
        "        \"\"\" Loads a csv file as a dataframe based on the path specified \"\"\"\n",
        "        options = {'format':'csv', 'header':header}\n",
        "        df = spark.read.load(self.to_url(source_path), **options)\n",
        "        return df      \n",
        "\n",
        "    def load_json(self, source_path, multiline=False):\n",
        "        \"\"\" Loads a json file as a dataframe based on the path specified \"\"\"\n",
        "        options = {'format':'json', 'multiline':multiline}\n",
        "        df = spark.read.load(self.to_url(source_path), **options)\n",
        "        return df    \n",
        "\n",
        "    def pseudonymize(self, df, metadata): #: list[list[str]]):\n",
        "        \"\"\" Performs pseudonymization of the given dataframe based on the provided metadata (in the OEA format).\n",
        "            For example, if the given df is for an entity called person, \n",
        "            2 dataframes will be returned, one called person that has hashed ids and masked fields, \n",
        "            and one called person_lookup that contains the original person_id, person_id_pseudo,\n",
        "            and the non-masked values for columns marked to be masked.           \n",
        "            The lookup table should be written to a \"sensitive\" folder in the data lake.\n",
        "            eg, df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\n",
        "            [More info on this approach here: https://learn.microsoft.com/en-us/azure/databricks/security/privacy/gdpr-delta#pseudonymize-data]\n",
        "        \"\"\"\n",
        "        df_pseudo = df\n",
        "        df_lookup = df\n",
        "        for col_name, dtype, op in metadata:\n",
        "            if op == \"hash-no-lookup\" or op == \"hnl\":\n",
        "                # This means that the lookup can be performed against a different table so no lookup is needed.\n",
        "                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
        "                df_lookup = df_lookup.drop(col_name)           \n",
        "            elif op == \"hash\" or op == 'h':\n",
        "                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
        "                df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256))\n",
        "            elif op == \"mask\" or op == 'm':\n",
        "                df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\n",
        "            elif op == \"partition-by\":\n",
        "                pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\n",
        "            elif op == \"no-op\" or op == 'x':\n",
        "                df_lookup = df_lookup.drop(col_name)\n",
        "        return (df_pseudo, df_lookup)\n",
        "\n",
        "    def add_to_lake_db(self, source_entity_path):\n",
        "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\n",
        "            This method will also create the lake db if it doesn't already exist.\n",
        "            eg: add_to_lake_db('ldb_s2_, 'stage2/Ingested/contoso_sis/v0.1/students')\n",
        "\n",
        "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
        "        \"\"\"\n",
        "        source_dict = self.parse_path(source_entity_path)\n",
        "        db_name = source_dict['ldb_name']\n",
        "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
        "        spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\n",
        "\n",
        "    def drop_lake_db(self, db_name):\n",
        "        spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
        "        result = \"Database dropped: \" + db_name\n",
        "        logger.info(result)\n",
        "        return result\n",
        "\n",
        "    def create_sql_db(self, source_path):\n",
        "        \"\"\" Prints out the sql script needed for creating a sql serverless db and set of views. \"\"\"\n",
        "        source_dict = self.parse_path(source_path)\n",
        "        db_name = source_dict['sdb_name']\n",
        "        cmd = '-- Create a new sql script then execute the following in it:\\n'\n",
        "        cmd += f\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{db_name}')\\nBEGIN\\n  CREATE DATABASE {db_name};\\nEND;\\nGO\\n\"\n",
        "        cmd += f\"USE {db_name};\\nGO\\n\\n\"\n",
        "        cmd += self.create_sql_views(source_dict['entity_parent_path'])\n",
        "        print(cmd)\n",
        "\n",
        "    def create_sql_views(self, source_path):\n",
        "        cmd = ''      \n",
        "        dirs = self.get_folders(source_path)\n",
        "        for table_name in dirs:\n",
        "            cmd += f\"CREATE OR ALTER VIEW {table_name} AS\\n  SELECT * FROM OPENROWSET(BULK '{self.to_url(source_path)}/{table_name}', FORMAT='delta') AS [r];\\nGO\\n\"\n",
        "        return cmd \n",
        "\n",
        "    def drop_sql_db(self, db_name):\n",
        "        cmd = '-- Create a new sql script then execute the following in it. Alternatively, you can click on the menu next to the SQL db and select \"Delete\"\\n'\n",
        "        cmd += '-- [Note that this does not affect the data in the data lake - this will only delete the sql db that points to that data.]\\n\\n'\n",
        "        cmd += f'DROP DATABASE {db_name}'\n",
        "        print(cmd)       \n",
        "\n",
        "class DataLakeWriter:\n",
        "    def __init__(self, root_destination):\n",
        "        self.root_destination = root_destination\n",
        "\n",
        "    def write(self, path_and_filename, data_str, format='csv'):\n",
        "        mssparkutils.fs.append(f\"{self.root_destination}/{path_and_filename}\", data_str, True) # Set the last parameter as True to create the file if it does not exist\n"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
