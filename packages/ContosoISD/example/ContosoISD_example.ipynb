{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ContosoISD Example\r\n",
        "This example demonstrates how to use the OEA framework and modules to process incoming data, perform data prep, and view the data in an example Power BI dashboard.\r\n",
        "\r\n",
        "# Running the example\r\n",
        "1) Select your spark pool in the \"Attach to\" dropdown list above.\r\n",
        "\r\n",
        "2) Click on \"Publish all\" in the top nav bar (to ensure all notebooks have been published)\r\n",
        "\r\n",
        "3) Click on \"Run all\" at the top of this tab.\r\n",
        "\r\n",
        "4) Open the dashboard in Power BI desktop and point it to your newly setup data lake (you can download the pbix from here: [techInequityDashboardContoso v2.pbix](https://github.com/microsoft/OpenEduAnalytics/blob/main/packages/ContosoISD/powerbi/techInequityDashboardContoso%20v2.pbix) )\r\n",
        "\r\n",
        "# More info\r\n",
        "See [ContosoISD package info](https://github.com/microsoft/OpenEduAnalytics/tree/move_to_delta/packages/ContosoISD) for more details on this example."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "%run /OEA_framework_example_py"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "StatementMeta(spark3p0smallc, 14, -1, Finished, Available)"
            ],
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p0smallc",
              "session_id": 14,
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-23T13:11:07.7805106Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-23T13:11:08.0214884Z",
              "execution_finish_time": "2021-08-23T13:11:08.0215913Z"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "%run /OEA_modules_example_py"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "StatementMeta(spark3p0smallc, 14, -1, Finished, Available)"
            ],
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p0smallc",
              "session_id": 14,
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-23T13:11:11.6411743Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-23T13:11:12.1006208Z",
              "execution_finish_time": "2021-08-23T13:11:12.1007362Z"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# 0) Initialize the OEA framework and modules needed.\r\n",
        "oea = OEA()\r\n",
        "m365 = M365(oea)\r\n",
        "contoso_sis = ContosoSIS(oea, 'contoso_sis', False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "StatementMeta(spark3p0smallc, 14, 5, Finished, Available)"
            ],
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p0smallc",
              "session_id": 14,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-23T13:11:14.0731821Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-23T13:11:14.17033Z",
              "execution_finish_time": "2021-08-23T13:11:16.0087912Z"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-08-23 13:11:15,381 - __main__ - DEBUG - OEA initialized."
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 1) Land data into stage1 of your data lake, from multiple source systems (this example copies in test data sets that came with the OEA installation).\r\n",
        "contoso_sis.copy_test_data_to_stage1()\r\n",
        "m365.copy_test_data_to_stage1()"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 2) Process the raw data (csv format) from stage1 into stage2 (adds schema details and writes out in parquet format).\r\n",
        "#    [Note: we're not performing pseudonymization in this example, so everything is written to container stage2np.]\r\n",
        "m365.process_roster_data_from_stage1()\r\n",
        "contoso_sis.process_data_from_stage1()\r\n",
        "m365.process_activity_data_from_stage1()"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 3) Run additional prep on the data to create a unified dataset that can be used in a Power BI report\r\n",
        "\r\n",
        "# Process sectionmark data. Convert id values to use the Person.Id and Section.Id values set in the m365 data.\r\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentsectionmark'), 'SectionMark')\r\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Person'), 'Person')\r\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Section'), 'Section')\r\n",
        "df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\r\n",
        "sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\r\n",
        "sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\r\n",
        "from SectionMark sm, Person p, Section s \\\r\n",
        "where sm.student_id = p.ExternalId \\\r\n",
        "and sm.section_id = s.ExternalId\")\r\n",
        "df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/SectionMark')\r\n",
        "\r\n",
        "# Repeat the above process, this time for student attendance\r\n",
        "# Convert id values to use the Person.Id, Org.Id and Section.Id values\r\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentattendance'), 'Attendance')\r\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Org'), 'Org')\r\n",
        "df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\r\n",
        "att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\r\n",
        "att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\r\n",
        "from Attendance att, Org o, Person p, Section s \\\r\n",
        "where att.student_id = p.ExternalId \\\r\n",
        "and att.school_id = o.ExternalId \\\r\n",
        "and att.section_id = s.ExternalId\")\r\n",
        "df.write.format('parquet').mode('overwrite').save(oea.stage2np +'/ContosoISD/Attendance')\r\n",
        "\r\n",
        "# Add 'Department' column to Course (hardcoded to \"Math\" for this Contoso example)\r\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Course'), 'Course')\r\n",
        "df = spark.sql(\"select Id, Name, Code, Description, ExternalId, CreateDate, LastModifiedDate, IsActive, CalendarId, 'Math' Department from Course\")\r\n",
        "df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/Course')"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 4) Create spark db's that point to the data in the data lake to allow for connecting via Power BI through use of the Serverless SQL endpoint.\r\n",
        "contoso_sis.create_stage2_db('PARQUET')\r\n",
        "m365.create_stage2_db('PARQUET')\r\n",
        "\r\n",
        "spark.sql('CREATE DATABASE IF NOT EXISTS s2_ContosoISD')\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.Activity using PARQUET location '\" + oea.stage2np + \"/m365/TechActivity'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.Calendar using PARQUET location '\" + oea.stage2np + \"/m365/Calendar'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.Org using PARQUET location '\" + oea.stage2np + \"/m365/Org'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.Person using PARQUET location '\" + oea.stage2np + \"/m365/Person'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.PersonIdentifier using PARQUET location '\" + oea.stage2np + \"/m365/PersonIdentifier'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.RefDefinition using PARQUET location '\" + oea.stage2np + \"/m365/RefDefinition'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.Section using PARQUET location '\" + oea.stage2np + \"/m365/Section'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.Session using PARQUET location '\" + oea.stage2np + \"/m365/Session'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.StaffOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StaffOrgAffiliation'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.StaffSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StaffSectionMembership'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.StudentOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StudentOrgAffiliation'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.StudentSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StudentSectionMembership'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.Course using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Course'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.Attendance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Attendance'\")\r\n",
        "spark.sql(f\"create table if not exists s2_ContosoISD.SectionMark using PARQUET location '\" + oea.stage2np + \"/ContosoISD/SectionMark'\")\r\n",
        "\r\n",
        "print(f\"Created spark db's.\\nYou can now open the 'techInequityDashboardContoso v2.pbix' dashboard and change the datasource to point to: {oea.serverless_sql_endpoint}\")"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reset everything\r\n",
        "You can uncomment line 11 in the cell below and run the cell to reset everything and walk through the process again from the top.\r\n",
        "\r\n",
        "Note: remember to comment out line 11 again to prevent accidental resetting of the example"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "def reset_all_processing():\r\n",
        "    contoso_sis.delete_all_stages()\r\n",
        "    m365.delete_all_stages()\r\n",
        "    oea.rm_if_exists(oea.stage2np + '/ContosoISD')\r\n",
        "\r\n",
        "    oea.drop_db('s2_contoso_sis')\r\n",
        "    oea.drop_db('s2_contosoisd')\r\n",
        "    oea.drop_db('s2_m365')\r\n",
        "\r\n",
        "# Uncomment the following line and run this cell to reset everything if you want to walk through the process again.\r\n",
        "reset_all_processing()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "StatementMeta(spark3p0smallc, 14, 6, Finished, Cancelled)"
            ],
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p0smallc",
              "session_id": 14,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "cancelled",
              "queued_time": "2021-08-23T13:11:20.4525136Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-23T13:11:20.5437762Z",
              "execution_finish_time": "2021-08-23T13:19:27.3378134Z"
            }
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}